{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "870cda79",
   "metadata": {},
   "source": [
    "# üöÄ POC: Busca Vetorial OTIMIZADA para AWS g4dn.2xlarge\n",
    "\n",
    "## üéØ Inst√¢ncia: g4dn.2xlarge\n",
    "- **GPU**: NVIDIA T4 (16GB VRAM) - Tensor Cores\n",
    "- **vCPUs**: 8 cores\n",
    "- **RAM**: 32GB\n",
    "- **Custo**: ~$0.75/h (on-demand) | ~$0.25/h (spot)\n",
    "\n",
    "## üöÄ Otimiza√ß√µes para T4:\n",
    "\n",
    "### 1. **Modelo de embedding otimizado:**\n",
    "- ‚úÖ `paraphrase-multilingual-MiniLM-L12-v2` (384 dims, 4x mais r√°pido)\n",
    "- ‚úÖ **Mixed Precision (FP16)**: 2x speedup nos Tensor Cores da T4\n",
    "- ‚úÖ **Batch size: 256** (aproveita 16GB VRAM vs 32 na CPU)\n",
    "\n",
    "### 2. **FAISS IndexHNSWFlat:**\n",
    "- ‚úÖ Sem treinamento (ao contr√°rio de IVF)\n",
    "- ‚úÖ **99.5% de precis√£o** (vs 95-98% IVF)\n",
    "- ‚úÖ **10-50x mais r√°pido** que Flat\n",
    "- ‚ö° Opcional: FAISS GPU (5-10x mais r√°pido na busca)\n",
    "\n",
    "### 3. **Performance esperada:**\n",
    "| M√©trica | CPU | GPU T4 |\n",
    "|---------|-----|--------|\n",
    "| Constru√ß√£o | 30-40min | **5-10min** |\n",
    "| Busca | 80-100ms | **30-50ms** |\n",
    "| Throughput | ~100 q/s | **~1000 q/s** |\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a67d97d",
   "metadata": {},
   "source": [
    "## üì¶ Setup e GPU Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aef84e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import re\n",
    "import faiss\n",
    "import pickle\n",
    "import torch\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Optional\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from unidecode import unidecode\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"üîç DETEC√á√ÉO DE HARDWARE\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Detectar GPU\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "if device == 'cuda':\n",
    "    gpu_name = torch.cuda.get_device_name(0)\n",
    "    gpu_memory = torch.cuda.get_device_properties(0).total_memory / (1024**3)\n",
    "    print(f\"\\nüéÆ GPU detectada: {gpu_name}\")\n",
    "    print(f\"üíæ VRAM total: {gpu_memory:.1f}GB\")\n",
    "    print(f\"‚úÖ CUDA version: {torch.version.cuda}\")\n",
    "    \n",
    "    # Verificar se √© T4\n",
    "    if 'T4' in gpu_name:\n",
    "        print(f\"üöÄ GPU T4 detectada - Tensor Cores dispon√≠veis!\")\n",
    "        print(f\"‚ö° Mixed Precision (FP16) ser√° ativado automaticamente\")\n",
    "    \n",
    "    # FAISS GPU dispon√≠vel?\n",
    "    try:\n",
    "        res = faiss.StandardGpuResources()\n",
    "        faiss_gpu_available = True\n",
    "        print(f\"‚úÖ FAISS GPU dispon√≠vel (busca 5-10x mais r√°pida)\")\n",
    "    except:\n",
    "        faiss_gpu_available = False\n",
    "        print(f\"‚ö†Ô∏è  FAISS CPU only (para GPU: pip install faiss-gpu)\")\n",
    "else:\n",
    "    print(f\"\\n‚ö†Ô∏è  Executando em CPU\")\n",
    "    print(f\"üí° Recomendado: usar inst√¢ncia g4dn.2xlarge com GPU T4\")\n",
    "    faiss_gpu_available = False\n",
    "\n",
    "print(f\"\\nüîß Device para embeddings: {device}\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15a39e46",
   "metadata": {},
   "source": [
    "## 1. EmbeddingService - Otimizado para T4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a48a9912",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmbeddingServiceGPU:\n",
    "    \"\"\"Servi√ßo otimizado para GPU NVIDIA T4 (g4dn.2xlarge)\"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self, \n",
    "        model_name: str = \"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\",\n",
    "        use_fp16: bool = True\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Modelos testados na T4 (velocidade vs qualidade):\n",
    "        \n",
    "        R√ÅPIDO (recomendado para produ√ß√£o):\n",
    "        - paraphrase-multilingual-MiniLM-L12-v2 (384 dims) ‚úÖ MELHOR CUSTO-BENEF√çCIO\n",
    "        - all-MiniLM-L6-v2 (384 dims, ingl√™s mas OK em PT)\n",
    "        \n",
    "        PRECISO (se precisar mais qualidade):\n",
    "        - paraphrase-multilingual-mpnet-base-v2 (768 dims, 2x mais lento)\n",
    "        - neuralmind/bert-base-portuguese-cased (768 dims, 3x mais lento)\n",
    "        \n",
    "        Args:\n",
    "            use_fp16: Mixed precision (FP16) - 2x speedup na T4 (Tensor Cores)\n",
    "        \"\"\"\n",
    "        print(f\"‚ö° Carregando modelo: {model_name}\")\n",
    "        self.model = SentenceTransformer(model_name, device=device)\n",
    "        \n",
    "        # Mixed precision para T4 (Tensor Cores)\n",
    "        if device == 'cuda' and use_fp16:\n",
    "            self.model.half()  # Converte para FP16\n",
    "            print(f\"üöÄ Mixed Precision (FP16) ATIVADO - 2x speedup\")\n",
    "        \n",
    "        self.embedding_dim = self.model.get_sentence_embedding_dimension()\n",
    "        print(f\"‚úÖ Embedding dimension: {self.embedding_dim}\")\n",
    "        print(f\"üîß Device: {self.model.device}\")\n",
    "        \n",
    "        # Batch size otimizado para T4 (16GB VRAM)\n",
    "        if device == 'cuda':\n",
    "            # T4 aguenta batch 256 com MiniLM (384 dims)\n",
    "            # Se usar modelo maior (768 dims), reduza para 128\n",
    "            self.optimal_batch_size = 256 if self.embedding_dim <= 384 else 128\n",
    "        else:\n",
    "            self.optimal_batch_size = 32\n",
    "        \n",
    "        print(f\"üì¶ Batch size otimizado: {self.optimal_batch_size}\")\n",
    "    \n",
    "    @staticmethod\n",
    "    def normalize_text(text: str) -> str:\n",
    "        \"\"\"Normaliza√ß√£o de endere√ßos brasileiros\"\"\"\n",
    "        if not text or not isinstance(text, str):\n",
    "            return \"\"\n",
    "        \n",
    "        text = unidecode(text)\n",
    "        text = text.lower()\n",
    "        \n",
    "        # Expandir abrevia√ß√µes (aceita com/sem ponto)\n",
    "        replacements = {\n",
    "            r'\\br\\.?\\s': 'rua ',\n",
    "            r'\\bav\\.?\\s': 'avenida ',\n",
    "            r'\\btrav\\.?\\s': 'travessa ',\n",
    "            r'\\balam\\.?\\s': 'alameda ',\n",
    "            r'\\bpca\\.?\\s': 'praca ',\n",
    "            r'\\bjd\\.?\\s': 'jardim ',\n",
    "            r'\\bvl\\.?\\s': 'vila ',\n",
    "            r'\\bcj\\.?\\s': 'conjunto ',\n",
    "            r'\\bqd\\.?\\s': 'quadra ',\n",
    "            r'\\blt\\.?\\s': 'lote ',\n",
    "        }\n",
    "        \n",
    "        for pattern, replacement in replacements.items():\n",
    "            text = re.sub(pattern, replacement, text)\n",
    "        \n",
    "        # Remover pontua√ß√£o e m√∫ltiplos espa√ßos\n",
    "        text = re.sub(r'[^\\w\\s]', ' ', text)\n",
    "        text = re.sub(r'\\s+', ' ', text).strip()\n",
    "        \n",
    "        return text\n",
    "    \n",
    "    def embed_text(self, text: str) -> np.ndarray:\n",
    "        \"\"\"Embedding de um texto (usado na busca)\"\"\"\n",
    "        normalized = self.normalize_text(text)\n",
    "        if not normalized:\n",
    "            return np.zeros(self.embedding_dim, dtype=np.float32)\n",
    "        return self.model.encode(\n",
    "            normalized, \n",
    "            convert_to_numpy=True, \n",
    "            show_progress_bar=False,\n",
    "            normalize_embeddings=True  # ‚Üê CR√çTICO: deve estar igual ao embed_batch!\n",
    "        )\n",
    "    \n",
    "    def embed_address_fields(self, address: Dict[str, str]) -> Dict[str, np.ndarray]:\n",
    "        \"\"\"Embedding de m√∫ltiplos campos de um endere√ßo\"\"\"\n",
    "        embeddings = {}\n",
    "        for field in ['logradouro', 'bairro', 'cidade']:\n",
    "            text = address.get(field, '')\n",
    "            embeddings[field] = self.embed_text(text)\n",
    "        return embeddings\n",
    "    \n",
    "    def embed_batch(self, texts, batch_size: int = None) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Embedding de batch otimizado para T4:\n",
    "        - CPU: batch=32 (limitado por RAM)\n",
    "        - GPU T4: batch=256 (16GB VRAM + FP16)\n",
    "        \"\"\"\n",
    "        if batch_size is None:\n",
    "            batch_size = self.optimal_batch_size\n",
    "        \n",
    "        # Converter Series para lista se necess√°rio\n",
    "        if hasattr(texts, 'tolist'):\n",
    "            texts = texts.tolist()\n",
    "        \n",
    "        # Normalizar todos os textos\n",
    "        normalized_texts = [self.normalize_text(t) for t in texts]\n",
    "        normalized_texts = [t if t else \" \" for t in normalized_texts]\n",
    "        \n",
    "        # Encode com configura√ß√µes otimizadas\n",
    "        embeddings = self.model.encode(\n",
    "            normalized_texts,\n",
    "            convert_to_numpy=True,\n",
    "            show_progress_bar=True,\n",
    "            batch_size=batch_size,\n",
    "            device=device,\n",
    "            normalize_embeddings=True  # L2 normalization (melhora similaridade)\n",
    "        )\n",
    "        \n",
    "        return embeddings.astype(np.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "722028f3",
   "metadata": {},
   "source": [
    "## 2. IndexBuilder - HNSW com suporte GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9cc048a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class IndexBuilderGPU:\n",
    "    \"\"\"Construtor de √≠ndices FAISS otimizado para T4\"\"\"\n",
    "    \n",
    "    def __init__(self, embedding_service: EmbeddingServiceGPU, use_gpu_index: bool = False):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            use_gpu_index: Transferir √≠ndices para GPU (busca 5-10x mais r√°pida)\n",
    "                          Requer: pip install faiss-gpu\n",
    "                          Aten√ß√£o: consome VRAM (pode conflitar com embeddings)\n",
    "        \"\"\"\n",
    "        self.embedding_service = embedding_service\n",
    "        self.indices = {}\n",
    "        self.dataframe = None\n",
    "        self.use_gpu_index = use_gpu_index and faiss_gpu_available\n",
    "        \n",
    "        if self.use_gpu_index:\n",
    "            self.gpu_resource = faiss.StandardGpuResources()\n",
    "            # Reservar 8GB para √≠ndices (deixa 8GB para embeddings)\n",
    "            self.gpu_resource.setTempMemory(8 * 1024 * 1024 * 1024)\n",
    "            print(f\"üéÆ FAISS GPU ativado (8GB reservados para √≠ndices)\")\n",
    "    \n",
    "    def build_indices(\n",
    "        self,\n",
    "        df: pd.DataFrame,\n",
    "        fields: list = None,\n",
    "        use_hnsw: bool = True,\n",
    "        M: int = 32,\n",
    "        efSearch: int = 32\n",
    "    ) -> dict:\n",
    "        \"\"\"\n",
    "        Constr√≥i √≠ndices FAISS HNSW otimizados\n",
    "        \n",
    "        Args:\n",
    "            use_hnsw: True=HNSW (r√°pido+preciso), False=Flat (lento)\n",
    "            M: Conex√µes no grafo HNSW (16-64)\n",
    "               - M=16: r√°pido, ~98% recall\n",
    "               - M=32: balanceado, ~99.5% recall ‚úÖ\n",
    "               - M=64: preciso, ~99.9% recall, +mem√≥ria\n",
    "            efSearch: Vizinhos na busca (16-64)\n",
    "               - 16: muito r√°pido\n",
    "               - 32: balanceado ‚úÖ\n",
    "               - 64: mais preciso\n",
    "        \"\"\"\n",
    "        if fields is None:\n",
    "            fields = ['logradouro', 'bairro', 'cidade']\n",
    "        \n",
    "        self.dataframe = df.copy()\n",
    "        n_records = len(df)\n",
    "        \n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"üî® CONSTRUINDO √çNDICES FAISS\")\n",
    "        print(f\"{'='*60}\")\n",
    "        print(f\"üìä Total de registros: {n_records:,}\")\n",
    "        print(f\"‚öôÔ∏è  Modo: {'HNSW (r√°pido+preciso)' if use_hnsw else 'Flat (lento)'}\")\n",
    "        print(f\"üéØ Par√¢metros: M={M}, efSearch={efSearch}\")\n",
    "        print(f\"\")\n",
    "        \n",
    "        total_start = time.time()\n",
    "        \n",
    "        for field in fields:\n",
    "            print(f\"\\nüìç Campo: {field}\")\n",
    "            print(f\"{'-'*40}\")\n",
    "            field_start = time.time()\n",
    "            \n",
    "            # Embedding do campo\n",
    "            texts = df[field].fillna('').astype(str).tolist()\n",
    "            print(f\"   ‚ö° Gerando embeddings...\")\n",
    "            embeddings = self.embedding_service.embed_batch(texts)\n",
    "            dimension = embeddings.shape[1]\n",
    "            \n",
    "            # Criar √≠ndice\n",
    "            if use_hnsw:\n",
    "                print(f\"   üß† Criando √≠ndice HNSW (dim={dimension})...\")\n",
    "                index_cpu = faiss.IndexHNSWFlat(dimension, M)\n",
    "                index_cpu.hnsw.efSearch = efSearch\n",
    "                \n",
    "                print(f\"   üì• Adicionando {n_records:,} vetores...\")\n",
    "                index_cpu.add(embeddings)\n",
    "                \n",
    "                # GPU (opcional)\n",
    "                if self.use_gpu_index:\n",
    "                    print(f\"   üéÆ Transferindo para GPU T4...\")\n",
    "                    try:\n",
    "                        index = faiss.index_cpu_to_gpu(self.gpu_resource, 0, index_cpu)\n",
    "                        print(f\"   ‚úÖ √çndice na GPU\")\n",
    "                    except Exception as e:\n",
    "                        print(f\"   ‚ö†Ô∏è  Falha GPU: {e}\")\n",
    "                        print(f\"   ‚ÑπÔ∏è  Usando √≠ndice CPU\")\n",
    "                        index = index_cpu\n",
    "                else:\n",
    "                    index = index_cpu\n",
    "            else:\n",
    "                print(f\"   üß† Criando √≠ndice Flat (dim={dimension})...\")\n",
    "                index = faiss.IndexFlatL2(dimension)\n",
    "                index.add(embeddings)\n",
    "                \n",
    "                if self.use_gpu_index:\n",
    "                    index = faiss.index_cpu_to_gpu(self.gpu_resource, 0, index)\n",
    "            \n",
    "            self.indices[field] = index\n",
    "            \n",
    "            elapsed = time.time() - field_start\n",
    "            print(f\"   ‚úÖ Conclu√≠do em {elapsed:.1f}s\")\n",
    "        \n",
    "        total_elapsed = time.time() - total_start\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"üéâ TODOS OS √çNDICES CONSTRU√çDOS\")\n",
    "        print(f\"‚è±Ô∏è  Tempo total: {total_elapsed/60:.1f}min\")\n",
    "        print(f\"{'='*60}\\n\")\n",
    "        \n",
    "        return self.indices\n",
    "    \n",
    "    def save_indices(self, output_dir: str):\n",
    "        \"\"\"Salva √≠ndices para reutiliza√ß√£o\"\"\"\n",
    "        output_path = Path(output_dir)\n",
    "        output_path.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        print(f\"\\nüíæ Salvando √≠ndices em: {output_path}\")\n",
    "        \n",
    "        for field, index in self.indices.items():\n",
    "            # Se √≠ndice est√° na GPU, transferir para CPU antes de salvar\n",
    "            if self.use_gpu_index:\n",
    "                index_cpu = faiss.index_gpu_to_cpu(index)\n",
    "            else:\n",
    "                index_cpu = index\n",
    "            \n",
    "            index_file = output_path / f\"{field}_index.faiss\"\n",
    "            faiss.write_index(index_cpu, str(index_file))\n",
    "            print(f\"   ‚úÖ {field}_index.faiss\")\n",
    "        \n",
    "        # Salvar dataframe\n",
    "        df_file = output_path / \"addresses.parquet\"\n",
    "        self.dataframe.to_parquet(df_file, index=False)\n",
    "        print(f\"   ‚úÖ addresses.parquet\")\n",
    "        \n",
    "        # Metadata\n",
    "        metadata = {\n",
    "            'fields': list(self.indices.keys()),\n",
    "            'n_records': len(self.dataframe),\n",
    "            'embedding_dim': self.embedding_service.embedding_dim\n",
    "        }\n",
    "        metadata_file = output_path / \"metadata.pkl\"\n",
    "        with open(metadata_file, 'wb') as f:\n",
    "            pickle.dump(metadata, f)\n",
    "        print(f\"   ‚úÖ metadata.pkl\")\n",
    "        \n",
    "        print(f\"\\nüéâ √çndices salvos! Use load_indices() para carregar.\")\n",
    "    \n",
    "    def load_indices(self, input_dir: str):\n",
    "        \"\"\"Carrega √≠ndices salvos (MUITO mais r√°pido)\"\"\"\n",
    "        input_path = Path(input_dir)\n",
    "        \n",
    "        print(f\"\\nüìÇ Carregando √≠ndices de: {input_path}\")\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # Metadata\n",
    "        metadata_file = input_path / \"metadata.pkl\"\n",
    "        with open(metadata_file, 'rb') as f:\n",
    "            metadata = pickle.load(f)\n",
    "        \n",
    "        # DataFrame\n",
    "        df_file = input_path / \"addresses.parquet\"\n",
    "        self.dataframe = pd.read_parquet(df_file)\n",
    "        \n",
    "        # √çndices\n",
    "        for field in metadata['fields']:\n",
    "            index_file = input_path / f\"{field}_index.faiss\"\n",
    "            index_cpu = faiss.read_index(str(index_file))\n",
    "            \n",
    "            # Transferir para GPU se ativado\n",
    "            if self.use_gpu_index:\n",
    "                try:\n",
    "                    index = faiss.index_cpu_to_gpu(self.gpu_resource, 0, index_cpu)\n",
    "                    print(f\"   ‚úÖ {field} (GPU)\")\n",
    "                except:\n",
    "                    index = index_cpu\n",
    "                    print(f\"   ‚úÖ {field} (CPU)\")\n",
    "            else:\n",
    "                index = index_cpu\n",
    "                print(f\"   ‚úÖ {field}\")\n",
    "            \n",
    "            self.indices[field] = index\n",
    "        \n",
    "        elapsed = time.time() - start_time\n",
    "        print(f\"\\n‚ö° Carregado em {elapsed:.1f}s ({len(self.dataframe):,} registros)\\n\")\n",
    "        \n",
    "        return self.indices, self.dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53b109c1",
   "metadata": {},
   "source": [
    "## 3. SearchEngine (mesmo da vers√£o anterior)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "399eb716",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SearchEngine:\n",
    "    \"\"\"Motor de busca vetorial com pesos din√¢micos\"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        embedding_service: EmbeddingServiceGPU,\n",
    "        indices: Dict[str, faiss.Index],\n",
    "        dataframe: pd.DataFrame\n",
    "    ):\n",
    "        self.embedding_service = embedding_service\n",
    "        self.indices = indices\n",
    "        self.dataframe = dataframe\n",
    "        \n",
    "        self.base_weights = {\n",
    "            'with_cep': {\n",
    "                'cep': 0.30,\n",
    "                'logradouro': 0.40,\n",
    "                'bairro': 0.20,\n",
    "                'cidade': 0.10\n",
    "            },\n",
    "            'without_cep': {\n",
    "                'logradouro': 0.55,\n",
    "                'bairro': 0.25,\n",
    "                'cidade': 0.20\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        self.use_uf_filter = True\n",
    "        self.confidence_threshold = 0.8\n",
    "    \n",
    "    def _get_dynamic_weights(self, query: Dict[str, str]) -> Dict[str, float]:\n",
    "        has_cep = bool(query.get('cep'))\n",
    "        weights = self.base_weights['with_cep' if has_cep else 'without_cep'].copy()\n",
    "        available_fields = [f for f in ['logradouro', 'bairro', 'cidade'] if query.get(f)]\n",
    "        filtered_weights = {k: v for k, v in weights.items() if k in available_fields or k == 'cep'}\n",
    "        total_weight = sum(filtered_weights.values())\n",
    "        if total_weight > 0:\n",
    "            normalized_weights = {k: v / total_weight for k, v in filtered_weights.items()}\n",
    "        else:\n",
    "            normalized_weights = filtered_weights\n",
    "        return normalized_weights\n",
    "    \n",
    "    def _calculate_field_similarity(\n",
    "        self,\n",
    "        field: str,\n",
    "        query_embedding: np.ndarray,\n",
    "        top_k: int = 100\n",
    "    ) -> tuple:\n",
    "        index = self.indices[field]\n",
    "        query_embedding = query_embedding.reshape(1, -1).astype(np.float32)\n",
    "        distances, indices = index.search(query_embedding, top_k)\n",
    "        similarities = 1.0 / (1.0 + distances[0])\n",
    "        return similarities, indices[0]\n",
    "    \n",
    "    def _calculate_cep_match(self, query_cep: str, db_cep: str) -> float:\n",
    "        if not query_cep or not db_cep:\n",
    "            return 0.0\n",
    "        \n",
    "        query_clean = query_cep.replace('-', '').replace('.', '')\n",
    "        db_clean = db_cep.replace('-', '').replace('.', '')\n",
    "        \n",
    "        if query_clean == db_clean:\n",
    "            return 1.0\n",
    "        \n",
    "        if len(query_clean) >= 5 and len(db_clean) >= 5:\n",
    "            if query_clean[:5] == db_clean[:5]:\n",
    "                return 0.5\n",
    "        \n",
    "        return 0.0\n",
    "    \n",
    "    def search(\n",
    "        self,\n",
    "        query: Dict[str, str],\n",
    "        top_k: int = 5,\n",
    "        search_k: int = 100\n",
    "    ) -> str:\n",
    "        weights = self._get_dynamic_weights(query)\n",
    "        query_embeddings = self.embedding_service.embed_address_fields(query)\n",
    "        \n",
    "        candidate_scores = {}\n",
    "        field_scores_map = {}\n",
    "        \n",
    "        for field in ['logradouro', 'bairro', 'cidade']:\n",
    "            if not query.get(field):\n",
    "                continue\n",
    "            \n",
    "            query_emb = query_embeddings[field]\n",
    "            similarities, indices = self._calculate_field_similarity(field, query_emb, search_k)\n",
    "            weight = weights.get(field, 0.0)\n",
    "            \n",
    "            for idx, sim in zip(indices, similarities):\n",
    "                if self.use_uf_filter and query.get('uf'):\n",
    "                    db_uf = self.dataframe.iloc[idx]['uf']\n",
    "                    if db_uf != query['uf']:\n",
    "                        continue\n",
    "                \n",
    "                if idx not in candidate_scores:\n",
    "                    candidate_scores[idx] = 0.0\n",
    "                    field_scores_map[idx] = {}\n",
    "                \n",
    "                candidate_scores[idx] += weight * sim\n",
    "                field_scores_map[idx][field] = float(sim)\n",
    "        \n",
    "        if query.get('cep'):\n",
    "            cep_weight = weights.get('cep', 0.0)\n",
    "            for idx in candidate_scores.keys():\n",
    "                db_cep = self.dataframe.iloc[idx]['cep']\n",
    "                cep_score = self._calculate_cep_match(query.get('cep'), db_cep)\n",
    "                candidate_scores[idx] += cep_weight * cep_score\n",
    "                field_scores_map[idx]['cep'] = cep_score\n",
    "        \n",
    "        sorted_candidates = sorted(\n",
    "            candidate_scores.items(),\n",
    "            key=lambda x: x[1],\n",
    "            reverse=True\n",
    "        )[:top_k]\n",
    "        \n",
    "        results = []\n",
    "        for idx, score in sorted_candidates:\n",
    "            row = self.dataframe.iloc[idx]\n",
    "            \n",
    "            if score >= self.confidence_threshold:\n",
    "                confidence = \"high\"\n",
    "            elif score >= 0.6:\n",
    "                confidence = \"medium\"\n",
    "            else:\n",
    "                confidence = \"low\"\n",
    "            \n",
    "            result = {\n",
    "                \"address\": {\n",
    "                    \"logradouro\": row['logradouro'],\n",
    "                    \"bairro\": row['bairro'],\n",
    "                    \"cidade\": row['cidade'],\n",
    "                    \"uf\": row['uf'],\n",
    "                    \"cep\": row['cep']\n",
    "                },\n",
    "                \"score\": float(score),\n",
    "                \"confidence\": confidence,\n",
    "                \"field_scores\": field_scores_map.get(idx, {})\n",
    "            }\n",
    "            results.append(result)\n",
    "        \n",
    "        response = {\n",
    "            \"results\": results,\n",
    "            \"query\": query,\n",
    "            \"total_found\": len(results),\n",
    "            \"weights_used\": weights\n",
    "        }\n",
    "        \n",
    "        return json.dumps(response, ensure_ascii=False, indent=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cfc0f2c",
   "metadata": {},
   "source": [
    "## 3.1. SearchEngineOptimized - Com Filtro de UF Otimizado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aea98da",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SearchEngineOptimized(SearchEngine):\n",
    "    \"\"\"\n",
    "    Motor de busca OTIMIZADO que filtra por UF ANTES da busca vetorial.\n",
    "    \n",
    "    Resolve o problema de candidatos de estados diferentes serem retornados\n",
    "    quando existem nomes de ruas/bairros/cidades duplicados em v√°rios estados.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        embedding_service: EmbeddingServiceGPU,\n",
    "        indices: Dict[str, faiss.Index],\n",
    "        dataframe: pd.DataFrame\n",
    "    ):\n",
    "        super().__init__(embedding_service, indices, dataframe)\n",
    "        \n",
    "        # Criar mapeamento de √≠ndices por UF para busca r√°pida\n",
    "        print(\"üó∫Ô∏è  Criando mapeamento de √≠ndices por UF...\")\n",
    "        self.uf_to_indices = {}\n",
    "        for uf in self.dataframe['uf'].unique():\n",
    "            uf_mask = self.dataframe['uf'] == uf\n",
    "            self.uf_to_indices[uf] = np.where(uf_mask)[0]\n",
    "        \n",
    "        total_ufs = len(self.uf_to_indices)\n",
    "        print(f\"‚úÖ Mapeamento criado: {total_ufs} UFs indexadas\")\n",
    "    \n",
    "    def _calculate_field_similarity(\n",
    "        self,\n",
    "        field: str,\n",
    "        query_embedding: np.ndarray,\n",
    "        top_k: int = 100\n",
    "    ) -> tuple:\n",
    "        \"\"\"Busca vetorial com convers√£o correta de dist√¢ncia para similaridade\"\"\"\n",
    "        index = self.indices[field]\n",
    "        query_embedding = query_embedding.reshape(1, -1).astype(np.float32)\n",
    "        distances, indices = index.search(query_embedding, top_k)\n",
    "        \n",
    "        # Converter dist√¢ncias L2¬≤ para similaridade cosseno\n",
    "        # Para vetores normalizados: ||a-b||¬≤ = 2(1 - cos(a,b))\n",
    "        # Portanto: cos(a,b) = 1 - (||a-b||¬≤ / 2)\n",
    "        similarities = np.clip(1.0 - (distances[0] / 2.0), 0.0, 1.0)\n",
    "        \n",
    "        # Considerar dist√¢ncias muito pequenas como match perfeito\n",
    "        epsilon = 1e-6\n",
    "        similarities[distances[0] < epsilon] = 1.0\n",
    "        \n",
    "        return similarities, indices[0]\n",
    "    \n",
    "    def search(\n",
    "        self,\n",
    "        query: Dict[str, str],\n",
    "        top_k: int = 5,\n",
    "        search_k: int = 100\n",
    "    ) -> str:\n",
    "        \"\"\"\n",
    "        Busca otimizada com filtro de UF aplicado ANTES da busca vetorial.\n",
    "        \n",
    "        Estrat√©gia:\n",
    "        1. Se UF fornecida: busca apenas nos √≠ndices daquele estado\n",
    "        2. Sem UF: busca global (comportamento original)\n",
    "        \"\"\"\n",
    "        weights = self._get_dynamic_weights(query)\n",
    "        query_embeddings = self.embedding_service.embed_address_fields(query)\n",
    "        \n",
    "        # Determinar o conjunto de √≠ndices v√°lidos (filtro de UF)\n",
    "        query_uf = query.get('uf')\n",
    "        if query_uf and query_uf in self.uf_to_indices:\n",
    "            valid_indices_set = set(self.uf_to_indices[query_uf])\n",
    "            print(f\"üîç Filtrando por UF={query_uf}: {len(valid_indices_set):,} registros\")\n",
    "        else:\n",
    "            valid_indices_set = None  # Busca global\n",
    "        \n",
    "        candidate_scores = {}\n",
    "        field_scores_map = {}\n",
    "        \n",
    "        # Busca vetorial por campo\n",
    "        for field in ['logradouro', 'bairro', 'cidade']:\n",
    "            if not query.get(field):\n",
    "                continue\n",
    "            \n",
    "            query_emb = query_embeddings[field]\n",
    "            \n",
    "            # Buscar mais candidatos para garantir que encontramos matches do UF correto\n",
    "            search_k_adjusted = search_k * 10 if valid_indices_set else search_k\n",
    "            similarities, indices = self._calculate_field_similarity(\n",
    "                field, query_emb, search_k_adjusted\n",
    "            )\n",
    "            weight = weights.get(field, 0.0)\n",
    "            \n",
    "            # Filtrar e pontuar candidatos\n",
    "            for idx, sim in zip(indices, similarities):\n",
    "                # Aplicar filtro de UF\n",
    "                if valid_indices_set is not None and idx not in valid_indices_set:\n",
    "                    continue\n",
    "                \n",
    "                # Acumular scores\n",
    "                if idx not in candidate_scores:\n",
    "                    candidate_scores[idx] = 0.0\n",
    "                    field_scores_map[idx] = {}\n",
    "                \n",
    "                candidate_scores[idx] += weight * sim\n",
    "                field_scores_map[idx][field] = float(sim)\n",
    "        \n",
    "        # CEP matching\n",
    "        if query.get('cep'):\n",
    "            cep_weight = weights.get('cep', 0.0)\n",
    "            for idx in candidate_scores.keys():\n",
    "                db_cep = self.dataframe.iloc[idx]['cep']\n",
    "                cep_score = self._calculate_cep_match(query.get('cep'), db_cep)\n",
    "                candidate_scores[idx] += cep_weight * cep_score\n",
    "                field_scores_map[idx]['cep'] = cep_score\n",
    "        \n",
    "        # Ordenar e retornar top_k\n",
    "        sorted_candidates = sorted(\n",
    "            candidate_scores.items(),\n",
    "            key=lambda x: x[1],\n",
    "            reverse=True\n",
    "        )[:top_k]\n",
    "        \n",
    "        # Montar resposta\n",
    "        results = []\n",
    "        for idx, score in sorted_candidates:\n",
    "            row = self.dataframe.iloc[idx]\n",
    "            \n",
    "            if score >= self.confidence_threshold:\n",
    "                confidence = \"high\"\n",
    "            elif score >= 0.6:\n",
    "                confidence = \"medium\"\n",
    "            else:\n",
    "                confidence = \"low\"\n",
    "            \n",
    "            result = {\n",
    "                \"address\": {\n",
    "                    \"logradouro\": row['logradouro'],\n",
    "                    \"bairro\": row['bairro'],\n",
    "                    \"cidade\": row['cidade'],\n",
    "                    \"uf\": row['uf'],\n",
    "                    \"cep\": row['cep']\n",
    "                },\n",
    "                \"score\": float(score),\n",
    "                \"confidence\": confidence,\n",
    "                \"field_scores\": field_scores_map.get(idx, {})\n",
    "            }\n",
    "            results.append(result)\n",
    "        \n",
    "        response = {\n",
    "            \"results\": results,\n",
    "            \"query\": query,\n",
    "            \"total_found\": len(results),\n",
    "            \"weights_used\": weights\n",
    "        }\n",
    "        \n",
    "        return json.dumps(response, ensure_ascii=False, indent=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e1b4376",
   "metadata": {},
   "source": [
    "## 4. Carregar DNE Real"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56e536a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carregar DNE - apenas colunas necess√°rias\n",
    "dne_path = Path('../data/dne.parquet')\n",
    "print(f\"üìÇ Carregando DNE de: {dne_path}\")\n",
    "\n",
    "# Selecionar apenas as colunas que precisamos (mais r√°pido e menos mem√≥ria)\n",
    "df_dne = pd.read_parquet(\n",
    "    dne_path,\n",
    "    columns=['logradouro_completo', 'bairro_completo', 'cidade_completo', 'uf', 'cep']\n",
    ")\n",
    "\n",
    "# Renomear colunas para formato esperado\n",
    "df_dne = df_dne.rename(columns={\n",
    "    'logradouro_completo': 'logradouro',\n",
    "    'bairro_completo': 'bairro',\n",
    "    'cidade_completo': 'cidade'\n",
    "})\n",
    "\n",
    "print(f\"\\n‚úÖ Dataset carregado: {len(df_dne):,} registros\")\n",
    "print(f\"üìã Colunas: {list(df_dne.columns)}\")\n",
    "print(f\"\\nüìä Distribui√ß√£o por UF:\")\n",
    "print(df_dne['uf'].value_counts().head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba11d9ab",
   "metadata": {},
   "source": [
    "## 5. Construir √çndices (EXECUTAR UMA VEZ) - ~5-10min na T4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7c37041",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inicializar servi√ßo de embeddings com FP16 (T4 optimization)\n",
    "embedding_service = EmbeddingServiceGPU(\n",
    "    model_name=\"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\",\n",
    "    use_fp16=True  # 2x speedup na T4\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af3923bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construir √≠ndices HNSW\n",
    "# use_gpu_index=False: √≠ndice fica na CPU (economiza VRAM para embeddings)\n",
    "# use_gpu_index=True: √≠ndice na GPU (busca 5-10x mais r√°pida, mas consome VRAM)\n",
    "index_builder = IndexBuilderGPU(embedding_service, use_gpu_index=False)\n",
    "\n",
    "# Par√¢metros balanceados para 1.5M registros:\n",
    "# M=32: ~99.5% recall\n",
    "# efSearch=32: balanceado entre velocidade e precis√£o\n",
    "indices = index_builder.build_indices(\n",
    "    df_dne,\n",
    "    use_hnsw=True,\n",
    "    M=32,\n",
    "    efSearch=32\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "033aac70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SALVAR √≠ndices para n√£o precisar reconstruir\n",
    "index_builder.save_indices('../data/indices_gpu_t4')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "863405ed",
   "metadata": {},
   "source": [
    "## 6. Carregar √çndices (R√ÅPIDO - use sempre) - ~5s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "713802e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carregar modelo\n",
    "embedding_service = EmbeddingServiceGPU(\n",
    "    model_name=\"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\",\n",
    "    use_fp16=True\n",
    ")\n",
    "\n",
    "# Carregar √≠ndices salvos\n",
    "index_builder = IndexBuilderGPU(embedding_service, use_gpu_index=False)\n",
    "indices, df_dne = index_builder.load_indices('../data/indices_gpu_t4')\n",
    "\n",
    "# Inicializar motor de busca OTIMIZADO (com filtro de UF inteligente)\n",
    "search_engine = SearchEngineOptimized(embedding_service, indices, df_dne)\n",
    "print(f\"üöÄ Sistema pronto! ({len(df_dne):,} endere√ßos indexados)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f812497",
   "metadata": {},
   "source": [
    "## 6.1. Teste R√°pido - Compara√ß√£o Antes/Depois"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a123f51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testar com o endere√ßo que estava retornando apenas 70%\n",
    "query_test = {\n",
    "    \"logradouro\": \"avenida rio branco\",\n",
    "    \"bairro\": \"cidade alta\",\n",
    "    \"cidade\": \"natal\",\n",
    "    \"uf\": \"RN\",\n",
    "    \"cep\": \"59025000\"\n",
    "}\n",
    "\n",
    "print(\"üéØ Teste com SearchEngineOptimized\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Query: {query_test}\\n\")\n",
    "\n",
    "start = time.time()\n",
    "result_json = search_engine.search(query_test, top_k=3, search_k=100)\n",
    "elapsed = time.time() - start\n",
    "result = json.loads(result_json)\n",
    "\n",
    "print(f\"\\n‚ö° Busca conclu√≠da em: {elapsed*1000:.1f}ms\\n\")\n",
    "print(\"‚úÖ Top 3 Resultados:\\n\")\n",
    "\n",
    "for i, res in enumerate(result['results'], 1):\n",
    "    addr = res['address']\n",
    "    print(f\"{i}. Score: {res['score']:.4f} ({res['score']*100:.2f}%) - Confidence: {res['confidence']}\")\n",
    "    print(f\"   {addr['logradouro']}\")\n",
    "    print(f\"   {addr['bairro']} - {addr['cidade']}/{addr['uf']}\")\n",
    "    print(f\"   CEP: {addr['cep']}\")\n",
    "    print(f\"   Field scores: \", end=\"\")\n",
    "    for field, score in res['field_scores'].items():\n",
    "        print(f\"{field}={score:.3f} \", end=\"\")\n",
    "    print(\"\\n\")\n",
    "\n",
    "# Verifica√ß√£o\n",
    "top_score = result['results'][0]['score']\n",
    "if top_score >= 0.98:\n",
    "    print(f\"üéâ SUCESSO! Score corrigido: {top_score:.4f} (‚â•98%)\")\n",
    "    print(f\"‚úÖ Problema de candidatos diferentes RESOLVIDO!\")\n",
    "else:\n",
    "    print(f\"‚ö†Ô∏è  Score obtido: {top_score:.4f}\")\n",
    "    print(f\"   (Esperado ‚â•0.98 para match perfeito)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e17335c3",
   "metadata": {},
   "source": [
    "## 7. Teste de Busca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9016c18f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exemplo de busca\n",
    "query = {\n",
    "    'logradouro': 'Rua das Flores',\n",
    "    'bairro': 'Centro',\n",
    "    'cidade': 'S√£o Paulo',\n",
    "    'uf': 'SP',\n",
    "    'cep': '01000-000'\n",
    "}\n",
    "\n",
    "print(\"‚è±Ô∏è  Testando performance da busca...\\n\")\n",
    "\n",
    "# Medir tempo\n",
    "start = time.time()\n",
    "result = search_engine.search(query, top_k=5)\n",
    "elapsed = time.time() - start\n",
    "\n",
    "print(f\"‚ö° Busca conclu√≠da em: {elapsed*1000:.1f}ms\")\n",
    "print(f\"\\nResultados:\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1b346c3",
   "metadata": {},
   "source": [
    "## 7.1. Teste com Endere√ßo Real (Match Perfeito)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "532cfbdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testar com o endere√ßo exato do exemplo (Natal/RN)\n",
    "query_test = {\n",
    "    \"logradouro\": \"avenida rio branco\",\n",
    "    \"bairro\": \"cidade alta\",\n",
    "    \"cidade\": \"natal\",\n",
    "    \"uf\": \"RN\",\n",
    "    \"cep\": \"59025000\"\n",
    "}\n",
    "\n",
    "print(\"üéØ Testando match perfeito com endere√ßo real da base...\\n\")\n",
    "print(\"üìç Query:\")\n",
    "print(f\"   {query_test['logradouro']}\")\n",
    "print(f\"   {query_test['bairro']} - {query_test['cidade']}/{query_test['uf']}\")\n",
    "print(f\"   CEP: {query_test['cep']}\\n\")\n",
    "\n",
    "# Buscar\n",
    "start = time.time()\n",
    "result_json = search_engine.search(query_test, top_k=3)\n",
    "elapsed = time.time() - start\n",
    "result = json.loads(result_json)\n",
    "\n",
    "# Mostrar resultado\n",
    "print(f\"‚ö° Busca conclu√≠da em: {elapsed*1000:.1f}ms\\n\")\n",
    "print(\"‚úÖ Top 3 Resultados:\\n\")\n",
    "\n",
    "for i, res in enumerate(result['results'], 1):\n",
    "    addr = res['address']\n",
    "    print(f\"{i}. Score: {res['score']:.4f} ({res['score']*100:.2f}%) - Confidence: {res['confidence']}\")\n",
    "    print(f\"   {addr['logradouro']}\")\n",
    "    print(f\"   {addr['bairro']} - {addr['cidade']}/{addr['uf']}\")\n",
    "    print(f\"   CEP: {addr['cep']}\")\n",
    "    print(f\"   Field scores: \", end=\"\")\n",
    "    for field, score in res['field_scores'].items():\n",
    "        print(f\"{field}={score:.3f} \", end=\"\")\n",
    "    print(\"\\n\")\n",
    "\n",
    "# Verifica√ß√£o\n",
    "top_score = result['results'][0]['score']\n",
    "if top_score >= 0.98:\n",
    "    print(f\"üéâ SUCESSO! Score correto: {top_score:.4f} (‚â•98%)\")\n",
    "else:\n",
    "    print(f\"‚ö†Ô∏è  Score esperado ‚â•0.98, obtido: {top_score:.4f}\")\n",
    "    print(f\"   Pesos usados: {result['weights_used']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87cc05a2",
   "metadata": {},
   "source": [
    "## 7.2. Diagn√≥stico: Por que n√£o bate 100%?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d47405ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Diagn√≥stico detalhado do problema de score\n",
    "print(\"üîç DIAGN√ìSTICO: Por que o score n√£o chega a 100%?\\n\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# 1. Verificar se o endere√ßo exato existe na base\n",
    "query_diagnostic = {\n",
    "    \"logradouro\": \"avenida rio branco\",\n",
    "    \"bairro\": \"cidade alta\",\n",
    "    \"cidade\": \"natal\",\n",
    "    \"uf\": \"RN\",\n",
    "    \"cep\": \"59025000\"\n",
    "}\n",
    "\n",
    "# Procurar correspond√™ncia EXATA no dataframe\n",
    "print(\"\\n1Ô∏è‚É£ Verificando se endere√ßo EXATO existe na base:\")\n",
    "print(\"-\"*60)\n",
    "\n",
    "# Normalizar os termos de busca\n",
    "from unidecode import unidecode\n",
    "\n",
    "def normalize_for_search(text):\n",
    "    if pd.isna(text):\n",
    "        return \"\"\n",
    "    return unidecode(str(text).lower().strip())\n",
    "\n",
    "# Filtrar por UF primeiro\n",
    "df_rn = df_dne[df_dne['uf'] == 'RN'].copy()\n",
    "print(f\"   Total de endere√ßos no RN: {len(df_rn):,}\")\n",
    "\n",
    "# Buscar por CEP\n",
    "cep_matches = df_rn[df_rn['cep'] == query_diagnostic['cep']]\n",
    "print(f\"\\n   Endere√ßos com CEP {query_diagnostic['cep']}: {len(cep_matches)}\")\n",
    "\n",
    "if len(cep_matches) > 0:\n",
    "    print(\"\\n   Primeiros 3 registros com este CEP:\")\n",
    "    for idx, row in cep_matches.head(3).iterrows():\n",
    "        print(f\"\\n   [{idx}]\")\n",
    "        print(f\"      Logradouro: {row['logradouro']}\")\n",
    "        print(f\"      Bairro: {row['bairro']}\")\n",
    "        print(f\"      Cidade: {row['cidade']}\")\n",
    "        print(f\"      CEP: {row['cep']}\")\n",
    "\n",
    "# Buscar match exato nos textos normalizados\n",
    "print(\"\\n2Ô∏è‚É£ Verificando normaliza√ß√£o dos campos:\")\n",
    "print(\"-\"*60)\n",
    "\n",
    "query_norm = {\n",
    "    'logradouro': normalize_for_search(query_diagnostic['logradouro']),\n",
    "    'bairro': normalize_for_search(query_diagnostic['bairro']),\n",
    "    'cidade': normalize_for_search(query_diagnostic['cidade'])\n",
    "}\n",
    "\n",
    "print(f\"   Query normalizada:\")\n",
    "print(f\"      Logradouro: '{query_norm['logradouro']}'\")\n",
    "print(f\"      Bairro: '{query_norm['bairro']}'\")\n",
    "print(f\"      Cidade: '{query_norm['cidade']}'\")\n",
    "\n",
    "# Verificar se existe match exato\n",
    "if len(cep_matches) > 0:\n",
    "    first_match = cep_matches.iloc[0]\n",
    "    db_norm = {\n",
    "        'logradouro': normalize_for_search(first_match['logradouro']),\n",
    "        'bairro': normalize_for_search(first_match['bairro']),\n",
    "        'cidade': normalize_for_search(first_match['cidade'])\n",
    "    }\n",
    "    \n",
    "    print(f\"\\n   Primeiro registro da base normalizado:\")\n",
    "    print(f\"      Logradouro: '{db_norm['logradouro']}'\")\n",
    "    print(f\"      Bairro: '{db_norm['bairro']}'\")\n",
    "    print(f\"      Cidade: '{db_norm['cidade']}'\")\n",
    "    \n",
    "    print(f\"\\n   Compara√ß√£o:\")\n",
    "    for field in ['logradouro', 'bairro', 'cidade']:\n",
    "        match = \"‚úÖ MATCH\" if query_norm[field] == db_norm[field] else f\"‚ùå DIFERENTE\"\n",
    "        print(f\"      {field}: {match}\")\n",
    "\n",
    "# 3. Testar busca campo por campo\n",
    "print(\"\\n3Ô∏è‚É£ Testando similaridade por campo individual:\")\n",
    "print(\"-\"*60)\n",
    "\n",
    "for field in ['logradouro', 'bairro', 'cidade']:\n",
    "    query_text = query_diagnostic[field]\n",
    "    query_emb = search_engine.embedding_service.embed_text(query_text)\n",
    "    \n",
    "    # Buscar top 3\n",
    "    similarities, indices = search_engine._calculate_field_similarity(field, query_emb, top_k=3)\n",
    "    \n",
    "    print(f\"\\n   Campo: {field}\")\n",
    "    print(f\"   Query: '{query_text}'\")\n",
    "    print(f\"   Top 3 resultados:\")\n",
    "    \n",
    "    for i, (sim, idx) in enumerate(zip(similarities, indices), 1):\n",
    "        db_value = df_dne.iloc[idx][field]\n",
    "        db_uf = df_dne.iloc[idx]['uf']\n",
    "        print(f\"      {i}. [{idx}] Score: {sim:.4f} | UF: {db_uf} | Valor: '{db_value}'\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üí° CONCLUS√ÉO:\")\n",
    "print(\"=\"*60)\n",
    "print(\"Se os campos est√£o batendo mas o score final √© baixo,\")\n",
    "print(\"o problema √© que a busca vetorial est√° retornando\")\n",
    "print(\"CANDIDATOS DIFERENTES para cada campo (logradouro, bairro, cidade).\")\n",
    "print(\"\\nSolu√ß√£o: os candidatos precisam se sobrepor (mesmo √≠ndice do dataframe)\")\n",
    "print(\"para que todos os scores sejam somados no mesmo resultado final.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88b017bc",
   "metadata": {},
   "source": [
    "## 8. Benchmark de Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4bccc28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testar m√∫ltiplas buscas\n",
    "n_searches = 50\n",
    "times = []\n",
    "\n",
    "print(f\"üî• Executando {n_searches} buscas para benchmark...\\n\")\n",
    "\n",
    "for i in tqdm(range(n_searches), desc=\"Buscas\"):\n",
    "    sample = df_dne.sample(1).iloc[0]\n",
    "    query = {\n",
    "        'logradouro': sample['logradouro'],\n",
    "        'bairro': sample['bairro'],\n",
    "        'cidade': sample['cidade'],\n",
    "        'uf': sample['uf']\n",
    "    }\n",
    "    \n",
    "    start = time.time()\n",
    "    result = search_engine.search(query, top_k=5)\n",
    "    elapsed = time.time() - start\n",
    "    times.append(elapsed)\n",
    "\n",
    "times_ms = [t * 1000 for t in times]\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"üìä ESTAT√çSTICAS DE PERFORMANCE\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"M√©dia:    {np.mean(times_ms):.1f}ms\")\n",
    "print(f\"Mediana:  {np.median(times_ms):.1f}ms\")\n",
    "print(f\"Min:      {np.min(times_ms):.1f}ms\")\n",
    "print(f\"Max:      {np.max(times_ms):.1f}ms\")\n",
    "print(f\"P95:      {np.percentile(times_ms, 95):.1f}ms\")\n",
    "print(f\"P99:      {np.percentile(times_ms, 99):.1f}ms\")\n",
    "print(f\"\\n‚ö° Throughput: ~{1000/np.mean(times_ms):.0f} queries/segundo\")\n",
    "print(f\"{'='*60}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "383c33cf",
   "metadata": {},
   "source": [
    "## üìù Notas de Otimiza√ß√£o para g4dn.2xlarge\n",
    "\n",
    "### Performance esperada na T4:\n",
    "| Opera√ß√£o | Tempo |\n",
    "|----------|-------|\n",
    "| Constru√ß√£o inicial (1.5M) | 5-10min |\n",
    "| Carregamento √≠ndices | <10s |\n",
    "| Busca (p50) | 30-50ms |\n",
    "| Busca (p95) | 60-80ms |\n",
    "| Throughput | ~500-1000 q/s |\n",
    "\n",
    "### Ajustes finos:\n",
    "\n",
    "**1. Modelo de embedding:**\n",
    "```python\n",
    "# R√ÅPIDO (atual)\n",
    "\"paraphrase-multilingual-MiniLM-L12-v2\"  # 384 dims, batch 256\n",
    "\n",
    "# MUITO R√ÅPIDO\n",
    "\"all-MiniLM-L6-v2\"  # 384 dims, batch 256, ingl√™s mas OK\n",
    "\n",
    "# PRECISO (mais lento)\n",
    "\"paraphrase-multilingual-mpnet-base-v2\"  # 768 dims, batch 128\n",
    "```\n",
    "\n",
    "**2. Par√¢metros HNSW:**\n",
    "```python\n",
    "# Mais r√°pido (98% recall)\n",
    "M=16, efSearch=16\n",
    "\n",
    "# Balanceado (99.5% recall) ‚úÖ\n",
    "M=32, efSearch=32\n",
    "\n",
    "# Mais preciso (99.9% recall)\n",
    "M=64, efSearch=64\n",
    "```\n",
    "\n",
    "**3. FAISS GPU (opcional):**\n",
    "```bash\n",
    "# Instalar faiss-gpu\n",
    "pip uninstall faiss-cpu\n",
    "pip install faiss-gpu\n",
    "```\n",
    "```python\n",
    "# Usar √≠ndices na GPU (busca 5-10x mais r√°pida)\n",
    "index_builder = IndexBuilderGPU(embedding_service, use_gpu_index=True)\n",
    "```\n",
    "‚ö†Ô∏è **Aten√ß√£o**: √çndices na GPU consomem VRAM (pode conflitar com embeddings)\n",
    "\n",
    "### Custos AWS:\n",
    "- **g4dn.2xlarge On-Demand**: ~$0.75/hora\n",
    "- **g4dn.2xlarge Spot**: ~$0.25/hora (70% desconto)\n",
    "- **Constru√ß√£o √∫nica**: $0.05 (5min spot)\n",
    "- **Alternativa**: g4dn.xlarge ($0.53/h) se n√£o precisa de 32GB RAM\n",
    "\n",
    "### Produ√ß√£o:\n",
    "1. **Construir √≠ndices uma vez** (5-10min)\n",
    "2. **Salvar em S3** ou volume persistente\n",
    "3. **Carregar na inicializa√ß√£o** (<10s)\n",
    "4. **API com FastAPI** + autoscaling\n",
    "5. **Monitoramento**: CloudWatch + logs de lat√™ncia"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
