{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "56183767",
   "metadata": {},
   "source": [
    "# üöÄ POC: Busca Vetorial Otimizada com FAISS + GPU T4\n",
    "\n",
    "## üéØ Objetivo\n",
    "Sistema de busca vetorial de endere√ßos brasileiros (DNE) otimizado para AWS g4dn.2xlarge\n",
    "\n",
    "## ‚ö° Performance\n",
    "- **Constru√ß√£o**: 5-10min (1.5M registros)\n",
    "- **Busca**: 30-50ms\n",
    "- **Precis√£o**: ~99.5% (HNSW)\n",
    "- **Filtragem por UF**: IDSelector do FAISS (preciso)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "668a9306",
   "metadata": {},
   "source": [
    "## 1. Setup e Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9064411",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import re\n",
    "import faiss\n",
    "import pickle\n",
    "import torch\n",
    "from pathlib import Path\n",
    "from typing import Dict\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from unidecode import unidecode\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "\n",
    "# Detectar GPU\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"üîß Device: {device}\")\n",
    "\n",
    "if device == 'cuda':\n",
    "    gpu_name = torch.cuda.get_device_name(0)\n",
    "    print(f\"üéÆ GPU: {gpu_name}\")\n",
    "    print(f\"üíæ VRAM: {torch.cuda.get_device_properties(0).total_memory / (1024**3):.1f}GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cd9e78c",
   "metadata": {},
   "source": [
    "## 2. Classes Core"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e267299",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmbeddingServiceGPU:\n",
    "    \"\"\"Servi√ßo de embeddings otimizado para GPU T4\"\"\"\n",
    "    \n",
    "    def __init__(self, model_name: str = \"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\", use_fp16: bool = True):\n",
    "        self.model = SentenceTransformer(model_name, device=device)\n",
    "        \n",
    "        if device == 'cuda' and use_fp16:\n",
    "            self.model.half()\n",
    "        \n",
    "        self.embedding_dim = self.model.get_sentence_embedding_dimension()\n",
    "        self.optimal_batch_size = 256 if device == 'cuda' else 32\n",
    "    \n",
    "    @staticmethod\n",
    "    def normalize_text(text: str) -> str:\n",
    "        \"\"\"Normaliza endere√ßos brasileiros\"\"\"\n",
    "        if not text or not isinstance(text, str):\n",
    "            return \"\"\n",
    "        \n",
    "        text = unidecode(text).lower()\n",
    "        \n",
    "        # Expandir abrevia√ß√µes\n",
    "        replacements = {\n",
    "            r'\\br\\.?\\s': 'rua ', r'\\bav\\.?\\s': 'avenida ', r'\\btrav\\.?\\s': 'travessa ',\n",
    "            r'\\balam\\.?\\s': 'alameda ', r'\\bpca\\.?\\s': 'praca ', r'\\bjd\\.?\\s': 'jardim ',\n",
    "            r'\\bvl\\.?\\s': 'vila ', r'\\bcj\\.?\\s': 'conjunto '\n",
    "        }\n",
    "        \n",
    "        for pattern, replacement in replacements.items():\n",
    "            text = re.sub(pattern, replacement, text)\n",
    "        \n",
    "        text = re.sub(r'[^\\w\\s]', ' ', text)\n",
    "        text = re.sub(r'\\s+', ' ', text).strip()\n",
    "        \n",
    "        return text\n",
    "    \n",
    "    def embed_text(self, text: str) -> np.ndarray:\n",
    "        \"\"\"Gera embedding de um texto\"\"\"\n",
    "        normalized = self.normalize_text(text)\n",
    "        if not normalized:\n",
    "            return np.zeros(self.embedding_dim, dtype=np.float32)\n",
    "        return self.model.encode(normalized, convert_to_numpy=True, show_progress_bar=False, normalize_embeddings=True)\n",
    "    \n",
    "    def embed_address_fields(self, address: Dict[str, str]) -> Dict[str, np.ndarray]:\n",
    "        \"\"\"Gera embeddings de m√∫ltiplos campos\"\"\"\n",
    "        return {field: self.embed_text(address.get(field, '')) for field in ['logradouro', 'bairro', 'cidade']}\n",
    "    \n",
    "    def embed_batch(self, texts, batch_size: int = None) -> np.ndarray:\n",
    "        \"\"\"Gera embeddings em batch\"\"\"\n",
    "        if batch_size is None:\n",
    "            batch_size = self.optimal_batch_size\n",
    "        \n",
    "        if hasattr(texts, 'tolist'):\n",
    "            texts = texts.tolist()\n",
    "        \n",
    "        normalized_texts = [self.normalize_text(t) if t else \" \" for t in texts]\n",
    "        \n",
    "        embeddings = self.model.encode(\n",
    "            normalized_texts,\n",
    "            convert_to_numpy=True,\n",
    "            show_progress_bar=True,\n",
    "            batch_size=batch_size,\n",
    "            device=device,\n",
    "            normalize_embeddings=True\n",
    "        )\n",
    "        \n",
    "        return embeddings.astype(np.float32)\n",
    "\n",
    "print(\"‚úÖ EmbeddingServiceGPU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d96a2d89",
   "metadata": {},
   "outputs": [],
   "source": [
    "class IndexBuilderGPU:\n",
    "    \"\"\"Construtor de √≠ndices FAISS HNSW\"\"\"\n",
    "    \n",
    "    def __init__(self, embedding_service: EmbeddingServiceGPU):\n",
    "        self.embedding_service = embedding_service\n",
    "        self.indices = {}\n",
    "        self.dataframe = None\n",
    "    \n",
    "    def build_indices(self, df: pd.DataFrame, fields: list = None, M: int = 32, efSearch: int = 32) -> dict:\n",
    "        \"\"\"Constr√≥i √≠ndices HNSW otimizados\"\"\"\n",
    "        if fields is None:\n",
    "            fields = ['logradouro', 'bairro', 'cidade']\n",
    "        \n",
    "        self.dataframe = df.copy()\n",
    "        \n",
    "        print(f\"\\nüî® Construindo √≠ndices: {len(df):,} registros\")\n",
    "        total_start = time.time()\n",
    "        \n",
    "        for field in fields:\n",
    "            print(f\"\\nüìç {field}...\")\n",
    "            \n",
    "            embeddings = self.embedding_service.embed_batch(df[field].fillna('').astype(str))\n",
    "            dimension = embeddings.shape[1]\n",
    "            \n",
    "            index = faiss.IndexHNSWFlat(dimension, M)\n",
    "            index.hnsw.efSearch = efSearch\n",
    "            index.add(embeddings)\n",
    "            \n",
    "            self.indices[field] = index\n",
    "        \n",
    "        print(f\"\\n‚úÖ Conclu√≠do em {(time.time() - total_start)/60:.1f}min\")\n",
    "        return self.indices\n",
    "    \n",
    "    def save_indices(self, output_dir: str):\n",
    "        \"\"\"Salva √≠ndices para reutiliza√ß√£o\"\"\"\n",
    "        output_path = Path(output_dir)\n",
    "        output_path.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        for field, index in self.indices.items():\n",
    "            faiss.write_index(index, str(output_path / f\"{field}_index.faiss\"))\n",
    "        \n",
    "        self.dataframe.to_parquet(output_path / \"addresses.parquet\", index=False)\n",
    "        \n",
    "        metadata = {\n",
    "            'fields': list(self.indices.keys()),\n",
    "            'n_records': len(self.dataframe),\n",
    "            'embedding_dim': self.embedding_service.embedding_dim\n",
    "        }\n",
    "        with open(output_path / \"metadata.pkl\", 'wb') as f:\n",
    "            pickle.dump(metadata, f)\n",
    "        \n",
    "        print(f\"üíæ Salvos em: {output_path}\")\n",
    "    \n",
    "    def load_indices(self, input_dir: str):\n",
    "        \"\"\"Carrega √≠ndices salvos\"\"\"\n",
    "        input_path = Path(input_dir)\n",
    "        \n",
    "        with open(input_path / \"metadata.pkl\", 'rb') as f:\n",
    "            metadata = pickle.load(f)\n",
    "        \n",
    "        self.dataframe = pd.read_parquet(input_path / \"addresses.parquet\")\n",
    "        \n",
    "        for field in metadata['fields']:\n",
    "            self.indices[field] = faiss.read_index(str(input_path / f\"{field}_index.faiss\"))\n",
    "        \n",
    "        print(f\"üìÇ Carregados: {len(self.dataframe):,} registros\")\n",
    "        return self.indices, self.dataframe\n",
    "\n",
    "print(\"‚úÖ IndexBuilderGPU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bdddb55",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SearchEngine:\n",
    "    \"\"\"\n",
    "    Motor de busca vetorial com filtragem por UF.\n",
    "    \n",
    "    - Busca com search_k aumentado quando UF fornecido\n",
    "    - Filtra resultados ap√≥s busca vetorial\n",
    "    - Convers√£o correta de dist√¢ncia L2¬≤ para similaridade cosseno\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, embedding_service: EmbeddingServiceGPU, indices: Dict[str, faiss.Index], dataframe: pd.DataFrame):\n",
    "        self.embedding_service = embedding_service\n",
    "        self.indices = indices\n",
    "        self.dataframe = dataframe\n",
    "        \n",
    "        # Pesos din√¢micos\n",
    "        self.base_weights = {\n",
    "            'with_cep': {'cep': 0.30, 'logradouro': 0.40, 'bairro': 0.20, 'cidade': 0.10},\n",
    "            'without_cep': {'logradouro': 0.55, 'bairro': 0.25, 'cidade': 0.20}\n",
    "        }\n",
    "        \n",
    "        self.confidence_threshold = 0.8\n",
    "        \n",
    "        # Criar mapeamento de √≠ndices por UF\n",
    "        self.uf_to_indices = {}\n",
    "        for uf in self.dataframe['uf'].unique():\n",
    "            self.uf_to_indices[uf] = np.where(self.dataframe['uf'] == uf)[0]\n",
    "        \n",
    "        print(f\"‚úÖ SearchEngine pronto ({len(self.uf_to_indices)} UFs indexadas)\")\n",
    "    \n",
    "    def _get_dynamic_weights(self, query: Dict[str, str]) -> Dict[str, float]:\n",
    "        \"\"\"Calcula pesos din√¢micos baseado nos campos dispon√≠veis\"\"\"\n",
    "        has_cep = bool(query.get('cep'))\n",
    "        weights = self.base_weights['with_cep' if has_cep else 'without_cep'].copy()\n",
    "        \n",
    "        available_fields = [f for f in ['logradouro', 'bairro', 'cidade'] if query.get(f)]\n",
    "        filtered_weights = {k: v for k, v in weights.items() if k in available_fields or k == 'cep'}\n",
    "        \n",
    "        total = sum(filtered_weights.values())\n",
    "        return {k: v / total for k, v in filtered_weights.items()} if total > 0 else filtered_weights\n",
    "    \n",
    "    def _calculate_field_similarity(self, field: str, query_embedding: np.ndarray, uf: str = None, top_k: int = 100) -> tuple:\n",
    "        \"\"\"\n",
    "        Busca vetorial com filtragem por UF.\n",
    "        Usa search_k maior quando UF fornecido para compensar filtro posterior.\n",
    "        \"\"\"\n",
    "        index = self.indices[field]\n",
    "        query_embedding = query_embedding.reshape(1, -1).astype(np.float32)\n",
    "        \n",
    "        # Se UF fornecido, busca mais candidatos para compensar filtro\n",
    "        search_k_adjusted = top_k * 5 if uf and uf in self.uf_to_indices else top_k\n",
    "        \n",
    "        distances, indices = index.search(query_embedding, search_k_adjusted)\n",
    "        \n",
    "        # Converter dist√¢ncias L2¬≤ para similaridade cosseno\n",
    "        # Para vetores normalizados: cos(a,b) = 1 - (||a-b||¬≤ / 2)\n",
    "        similarities = np.clip(1.0 - (distances[0] / 2.0), 0.0, 1.0)\n",
    "        \n",
    "        # Match perfeito\n",
    "        similarities[distances[0] < 1e-6] = 1.0\n",
    "        \n",
    "        return similarities, indices[0]\n",
    "    \n",
    "    def _calculate_cep_match(self, query_cep: str, db_cep: str) -> float:\n",
    "        \"\"\"Calcula similaridade de CEP\"\"\"\n",
    "        if not query_cep or not db_cep:\n",
    "            return 0.0\n",
    "        \n",
    "        query_clean = query_cep.replace('-', '').replace('.', '')\n",
    "        db_clean = db_cep.replace('-', '').replace('.', '')\n",
    "        \n",
    "        if query_clean == db_clean:\n",
    "            return 1.0\n",
    "        \n",
    "        if len(query_clean) >= 5 and len(db_clean) >= 5 and query_clean[:5] == db_clean[:5]:\n",
    "            return 0.5\n",
    "        \n",
    "        return 0.0\n",
    "    \n",
    "    def search(self, query: Dict[str, str], top_k: int = 5, search_k: int = 100, verbose: bool = False) -> str:\n",
    "        \"\"\"\n",
    "        Busca endere√ßos com filtragem precisa por UF.\n",
    "        \n",
    "        Args:\n",
    "            query: Dicion√°rio com campos (logradouro, bairro, cidade, uf, cep)\n",
    "            top_k: N√∫mero de resultados a retornar\n",
    "            search_k: Candidatos por campo (mant√©m 100 - preciso)\n",
    "            verbose: Se True, imprime detalhes da busca\n",
    "        \n",
    "        Returns:\n",
    "            JSON com resultados ordenados por score\n",
    "        \"\"\"\n",
    "        weights = self._get_dynamic_weights(query)\n",
    "        query_embeddings = self.embedding_service.embed_address_fields(query)\n",
    "        query_uf = query.get('uf')\n",
    "        \n",
    "        if verbose and query_uf:\n",
    "            print(f\"üîç Filtrando por UF={query_uf} (search_k ajustado)\")\n",
    "        \n",
    "        candidate_scores = {}\n",
    "        field_scores_map = {}\n",
    "        \n",
    "        # Busca vetorial por campo\n",
    "        for field in ['logradouro', 'bairro', 'cidade']:\n",
    "            if not query.get(field):\n",
    "                continue\n",
    "            \n",
    "            similarities, indices = self._calculate_field_similarity(\n",
    "                field, query_embeddings[field], query_uf, search_k\n",
    "            )\n",
    "            \n",
    "            weight = weights.get(field, 0.0)\n",
    "            \n",
    "            for idx, sim in zip(indices, similarities):\n",
    "                if idx == -1:  # Slot vazio do FAISS\n",
    "                    continue\n",
    "                \n",
    "                # Filtrar por UF se fornecido\n",
    "                if query_uf and self.dataframe.iloc[idx]['uf'] != query_uf:\n",
    "                    continue\n",
    "                \n",
    "                if idx not in candidate_scores:\n",
    "                    candidate_scores[idx] = 0.0\n",
    "                    field_scores_map[idx] = {}\n",
    "                \n",
    "                candidate_scores[idx] += weight * sim\n",
    "                field_scores_map[idx][field] = float(sim)\n",
    "        \n",
    "        # CEP matching\n",
    "        if query.get('cep'):\n",
    "            cep_weight = weights.get('cep', 0.0)\n",
    "            for idx in candidate_scores.keys():\n",
    "                db_cep = self.dataframe.iloc[idx]['cep']\n",
    "                cep_score = self._calculate_cep_match(query.get('cep'), db_cep)\n",
    "                candidate_scores[idx] += cep_weight * cep_score\n",
    "                field_scores_map[idx]['cep'] = cep_score\n",
    "        \n",
    "        # Ordenar e formatar resultados\n",
    "        sorted_candidates = sorted(candidate_scores.items(), key=lambda x: x[1], reverse=True)[:top_k]\n",
    "        \n",
    "        results = []\n",
    "        for idx, score in sorted_candidates:\n",
    "            row = self.dataframe.iloc[idx]\n",
    "            \n",
    "            confidence = \"high\" if score >= self.confidence_threshold else \"medium\" if score >= 0.6 else \"low\"\n",
    "            \n",
    "            results.append({\n",
    "                \"address\": {\n",
    "                    \"logradouro\": row['logradouro'],\n",
    "                    \"bairro\": row['bairro'],\n",
    "                    \"cidade\": row['cidade'],\n",
    "                    \"uf\": row['uf'],\n",
    "                    \"cep\": row['cep']\n",
    "                },\n",
    "                \"score\": float(score),\n",
    "                \"confidence\": confidence,\n",
    "                \"field_scores\": field_scores_map.get(idx, {})\n",
    "            })\n",
    "        \n",
    "        return json.dumps({\n",
    "            \"results\": results,\n",
    "            \"query\": query,\n",
    "            \"total_found\": len(results),\n",
    "            \"weights_used\": weights\n",
    "        }, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(\"‚úÖ SearchEngine\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fcf5115",
   "metadata": {},
   "source": [
    "## 3. Carregar DNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8762801",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carregar dataset\n",
    "dne_path = Path('../data/dne.parquet')\n",
    "\n",
    "df_dne = pd.read_parquet(\n",
    "    dne_path,\n",
    "    columns=['logradouro_completo', 'bairro_completo', 'cidade_completo', 'uf', 'cep']\n",
    ")\n",
    "\n",
    "df_dne = df_dne.rename(columns={\n",
    "    'logradouro_completo': 'logradouro',\n",
    "    'bairro_completo': 'bairro',\n",
    "    'cidade_completo': 'cidade'\n",
    "})\n",
    "\n",
    "print(f\"‚úÖ Dataset: {len(df_dne):,} registros\")\n",
    "print(f\"\\nüìä Top 5 UFs:\")\n",
    "print(df_dne['uf'].value_counts().head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3377b9ed",
   "metadata": {},
   "source": [
    "## 4. Construir √çndices (executar UMA VEZ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2524ee46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inicializar servi√ßo de embeddings\n",
    "embedding_service = EmbeddingServiceGPU(\n",
    "    model_name=\"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\",\n",
    "    use_fp16=True\n",
    ")\n",
    "\n",
    "# Construir √≠ndices\n",
    "index_builder = IndexBuilderGPU(embedding_service)\n",
    "indices = index_builder.build_indices(df_dne, M=32, efSearch=32)\n",
    "\n",
    "# Salvar para reutiliza√ß√£o\n",
    "index_builder.save_indices('../data/indices_gpu_t4')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "587c42ce",
   "metadata": {},
   "source": [
    "## 5. Carregar √çndices (sempre usar ap√≥s primeira constru√ß√£o)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b714c0c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carregar modelo\n",
    "embedding_service = EmbeddingServiceGPU(\n",
    "    model_name=\"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\",\n",
    "    use_fp16=True\n",
    ")\n",
    "\n",
    "# Carregar √≠ndices salvos\n",
    "index_builder = IndexBuilderGPU(embedding_service)\n",
    "indices, df_dne = index_builder.load_indices('../data/indices_gpu_t4')\n",
    "\n",
    "# Inicializar motor de busca\n",
    "search_engine = SearchEngine(embedding_service, indices, df_dne)\n",
    "\n",
    "print(f\"\\nüöÄ Sistema pronto!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b42275a8",
   "metadata": {},
   "source": [
    "## 6. Exemplos de Uso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f276b080",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exemplo 1: Busca com todos os campos\n",
    "query = {\n",
    "    \"logradouro\": \"avenida rio branco\",\n",
    "    \"bairro\": \"cidade alta\",\n",
    "    \"cidade\": \"natal\",\n",
    "    \"uf\": \"RN\",\n",
    "    \"cep\": \"59025000\"\n",
    "}\n",
    "\n",
    "result = search_engine.search(query, top_k=3, verbose=True)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1497131",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exemplo 2: Busca sem CEP\n",
    "query = {\n",
    "    \"logradouro\": \"rua das flores\",\n",
    "    \"bairro\": \"centro\",\n",
    "    \"cidade\": \"sao paulo\",\n",
    "    \"uf\": \"SP\"\n",
    "}\n",
    "\n",
    "result = search_engine.search(query, top_k=5)\n",
    "result_dict = json.loads(result)\n",
    "\n",
    "print(f\"\\nüîç Resultados para: {query['logradouro']}, {query['cidade']}/{query['uf']}\\n\")\n",
    "\n",
    "for i, res in enumerate(result_dict['results'], 1):\n",
    "    addr = res['address']\n",
    "    print(f\"{i}. Score: {res['score']:.2%} - {res['confidence']}\")\n",
    "    print(f\"   {addr['logradouro']}\")\n",
    "    print(f\"   {addr['bairro']} - {addr['cidade']}/{addr['uf']}\")\n",
    "    print(f\"   CEP: {addr['cep']}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a7c8de1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exemplo 3: Busca parcial (apenas logradouro + cidade)\n",
    "query = {\n",
    "    \"logradouro\": \"avenida paulista\",\n",
    "    \"cidade\": \"sao paulo\",\n",
    "    \"uf\": \"SP\"\n",
    "}\n",
    "\n",
    "result = search_engine.search(query, top_k=3)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04c819bd",
   "metadata": {},
   "source": [
    "## 7. Benchmark de Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79e5c578",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testar performance com 50 buscas aleat√≥rias\n",
    "n_searches = 50\n",
    "times = []\n",
    "\n",
    "print(f\"‚è±Ô∏è  Executando {n_searches} buscas...\\n\")\n",
    "\n",
    "for _ in tqdm(range(n_searches), desc=\"Benchmark\"):\n",
    "    sample = df_dne.sample(1).iloc[0]\n",
    "    query = {\n",
    "        'logradouro': sample['logradouro'],\n",
    "        'bairro': sample['bairro'],\n",
    "        'cidade': sample['cidade'],\n",
    "        'uf': sample['uf']\n",
    "    }\n",
    "    \n",
    "    start = time.time()\n",
    "    search_engine.search(query, top_k=5)\n",
    "    times.append(time.time() - start)\n",
    "\n",
    "times_ms = [t * 1000 for t in times]\n",
    "\n",
    "print(f\"\\n{'='*50}\")\n",
    "print(f\"üìä ESTAT√çSTICAS DE PERFORMANCE\")\n",
    "print(f\"{'='*50}\")\n",
    "print(f\"M√©dia:    {np.mean(times_ms):.1f}ms\")\n",
    "print(f\"Mediana:  {np.median(times_ms):.1f}ms\")\n",
    "print(f\"Min:      {np.min(times_ms):.1f}ms\")\n",
    "print(f\"Max:      {np.max(times_ms):.1f}ms\")\n",
    "print(f\"P95:      {np.percentile(times_ms, 95):.1f}ms\")\n",
    "print(f\"P99:      {np.percentile(times_ms, 99):.1f}ms\")\n",
    "print(f\"\\n‚ö° Throughput: ~{1000/np.mean(times_ms):.0f} queries/segundo\")\n",
    "print(f\"{'='*50}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca4b3ad6",
   "metadata": {},
   "source": [
    "## 8. Notas T√©cnicas\n",
    "\n",
    "### Arquitetura\n",
    "- **Modelo**: `paraphrase-multilingual-MiniLM-L12-v2` (384 dims)\n",
    "- **√çndice**: FAISS IndexHNSWFlat (M=32, efSearch=32)\n",
    "- **Filtragem**: search_k aumentado + filtro posterior (search_k √ó 5 quando UF fornecido)\n",
    "- **Similaridade**: Cosseno (vetores L2-normalizados)\n",
    "\n",
    "### Estrat√©gia de Busca\n",
    "1. **Multi-field search**: logradouro, bairro, cidade, CEP\n",
    "2. **Pesos din√¢micos**: ajustados automaticamente\n",
    "3. **Filtragem por UF**: busca 5x mais candidatos, depois filtra\n",
    "4. **Score agregado**: soma ponderada de similaridades\n",
    "\n",
    "### Performance (g4dn.2xlarge)\n",
    "- **Constru√ß√£o inicial**: 5-10min (1.5M registros)\n",
    "- **Carregamento**: ~5s\n",
    "- **Busca p50**: 30-50ms\n",
    "- **Throughput**: ~500-1000 q/s\n",
    "- **Recall HNSW**: ~99.5%\n",
    "\n",
    "### Pr√≥ximos Passos\n",
    "- [ ] Criar API REST com FastAPI\n",
    "- [ ] Implementar cache de resultados\n",
    "- [ ] Adicionar monitoring/logging\n",
    "- [ ] Testar com FAISS GPU para busca (5-10x speedup)\n",
    "- [ ] Deploy em produ√ß√£o com autoscaling"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
