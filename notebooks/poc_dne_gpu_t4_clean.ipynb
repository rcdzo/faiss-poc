{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "56183767",
   "metadata": {},
   "source": [
    "# üöÄ POC: Busca Vetorial Otimizada com FAISS + GPU T4\n",
    "\n",
    "## üéØ Objetivo\n",
    "Sistema de busca vetorial de endere√ßos brasileiros (DNE) otimizado para AWS g4dn.2xlarge\n",
    "\n",
    "## ‚ö° Performance\n",
    "- **Constru√ß√£o**: 5-10min (1.5M registros)\n",
    "- **Busca**: 30-50ms\n",
    "- **Precis√£o**: ~99.5% (HNSW)\n",
    "- **Filtragem por UF**: IDSelector do FAISS (preciso)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "668a9306",
   "metadata": {},
   "source": [
    "## 1. Setup e Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b9064411",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîß Device: cpu\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import re\n",
    "import faiss\n",
    "import pickle\n",
    "import torch\n",
    "from pathlib import Path\n",
    "from typing import Dict\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from unidecode import unidecode\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "\n",
    "# Detectar GPU\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"üîß Device: {device}\")\n",
    "\n",
    "if device == 'cuda':\n",
    "    gpu_name = torch.cuda.get_device_name(0)\n",
    "    print(f\"üéÆ GPU: {gpu_name}\")\n",
    "    print(f\"üíæ VRAM: {torch.cuda.get_device_properties(0).total_memory / (1024**3):.1f}GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cd9e78c",
   "metadata": {},
   "source": [
    "## 2. Classes Core"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7e267299",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ EmbeddingServiceGPU\n"
     ]
    }
   ],
   "source": [
    "class EmbeddingServiceGPU:\n",
    "    \"\"\"Servi√ßo de embeddings otimizado para GPU T4\"\"\"\n",
    "    \n",
    "    def __init__(self, model_name: str = \"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\", use_fp16: bool = True):\n",
    "        self.model = SentenceTransformer(model_name, device=device)\n",
    "        \n",
    "        if device == 'cuda' and use_fp16:\n",
    "            self.model.half()\n",
    "        \n",
    "        self.embedding_dim = self.model.get_sentence_embedding_dimension()\n",
    "        self.optimal_batch_size = 256 if device == 'cuda' else 32\n",
    "    \n",
    "    @staticmethod\n",
    "    def normalize_text(text: str) -> str:\n",
    "        \"\"\"Normaliza endere√ßos brasileiros\"\"\"\n",
    "        if not text or not isinstance(text, str):\n",
    "            return \"\"\n",
    "        \n",
    "        text = unidecode(text).lower()\n",
    "        \n",
    "        # Expandir abrevia√ß√µes\n",
    "        replacements = {\n",
    "            r'\\br\\.?\\s': 'rua ', r'\\bav\\.?\\s': 'avenida ', r'\\btrav\\.?\\s': 'travessa ',\n",
    "            r'\\balam\\.?\\s': 'alameda ', r'\\bpca\\.?\\s': 'praca ', r'\\bjd\\.?\\s': 'jardim ',\n",
    "            r'\\bvl\\.?\\s': 'vila ', r'\\bcj\\.?\\s': 'conjunto '\n",
    "        }\n",
    "        \n",
    "        for pattern, replacement in replacements.items():\n",
    "            text = re.sub(pattern, replacement, text)\n",
    "        \n",
    "        text = re.sub(r'[^\\w\\s]', ' ', text)\n",
    "        text = re.sub(r'\\s+', ' ', text).strip()\n",
    "        \n",
    "        return text\n",
    "    \n",
    "    def embed_text(self, text: str) -> np.ndarray:\n",
    "        \"\"\"Gera embedding de um texto\"\"\"\n",
    "        normalized = self.normalize_text(text)\n",
    "        if not normalized:\n",
    "            return np.zeros(self.embedding_dim, dtype=np.float32)\n",
    "        return self.model.encode(normalized, convert_to_numpy=True, show_progress_bar=False, normalize_embeddings=True)\n",
    "    \n",
    "    def embed_address_fields(self, address: Dict[str, str]) -> Dict[str, np.ndarray]:\n",
    "        \"\"\"Gera embeddings de m√∫ltiplos campos\"\"\"\n",
    "        return {field: self.embed_text(address.get(field, '')) for field in ['logradouro', 'bairro', 'cidade']}\n",
    "    \n",
    "    def embed_batch(self, texts, batch_size: int = None) -> np.ndarray:\n",
    "        \"\"\"Gera embeddings em batch\"\"\"\n",
    "        if batch_size is None:\n",
    "            batch_size = self.optimal_batch_size\n",
    "        \n",
    "        if hasattr(texts, 'tolist'):\n",
    "            texts = texts.tolist()\n",
    "        \n",
    "        normalized_texts = [self.normalize_text(t) if t else \" \" for t in texts]\n",
    "        \n",
    "        embeddings = self.model.encode(\n",
    "            normalized_texts,\n",
    "            convert_to_numpy=True,\n",
    "            show_progress_bar=True,\n",
    "            batch_size=batch_size,\n",
    "            device=device,\n",
    "            normalize_embeddings=True\n",
    "        )\n",
    "        \n",
    "        return embeddings.astype(np.float32)\n",
    "\n",
    "print(\"‚úÖ EmbeddingServiceGPU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d96a2d89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ IndexBuilderGPU\n"
     ]
    }
   ],
   "source": [
    "class IndexBuilderGPU:\n",
    "    \"\"\"Construtor de √≠ndices FAISS HNSW\"\"\"\n",
    "    \n",
    "    def __init__(self, embedding_service: EmbeddingServiceGPU):\n",
    "        self.embedding_service = embedding_service\n",
    "        self.indices = {}\n",
    "        self.dataframe = None\n",
    "    \n",
    "    def build_indices(self, df: pd.DataFrame, fields: list = None, M: int = 32, efSearch: int = 32) -> dict:\n",
    "        \"\"\"Constr√≥i √≠ndices HNSW otimizados\"\"\"\n",
    "        if fields is None:\n",
    "            fields = ['logradouro', 'bairro', 'cidade']\n",
    "        \n",
    "        self.dataframe = df.copy()\n",
    "        \n",
    "        print(f\"\\nüî® Construindo √≠ndices: {len(df):,} registros\")\n",
    "        total_start = time.time()\n",
    "        \n",
    "        for field in fields:\n",
    "            print(f\"\\nüìç {field}...\")\n",
    "            \n",
    "            embeddings = self.embedding_service.embed_batch(df[field].fillna('').astype(str))\n",
    "            dimension = embeddings.shape[1]\n",
    "            \n",
    "            index = faiss.IndexHNSWFlat(dimension, M)\n",
    "            index.hnsw.efSearch = efSearch\n",
    "            index.add(embeddings)\n",
    "            \n",
    "            self.indices[field] = index\n",
    "        \n",
    "        print(f\"\\n‚úÖ Conclu√≠do em {(time.time() - total_start)/60:.1f}min\")\n",
    "        return self.indices\n",
    "    \n",
    "    def save_indices(self, output_dir: str):\n",
    "        \"\"\"Salva √≠ndices para reutiliza√ß√£o\"\"\"\n",
    "        output_path = Path(output_dir)\n",
    "        output_path.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        for field, index in self.indices.items():\n",
    "            faiss.write_index(index, str(output_path / f\"{field}_index.faiss\"))\n",
    "        \n",
    "        self.dataframe.to_parquet(output_path / \"addresses.parquet\", index=False)\n",
    "        \n",
    "        metadata = {\n",
    "            'fields': list(self.indices.keys()),\n",
    "            'n_records': len(self.dataframe),\n",
    "            'embedding_dim': self.embedding_service.embedding_dim\n",
    "        }\n",
    "        with open(output_path / \"metadata.pkl\", 'wb') as f:\n",
    "            pickle.dump(metadata, f)\n",
    "        \n",
    "        print(f\"üíæ Salvos em: {output_path}\")\n",
    "    \n",
    "    def load_indices(self, input_dir: str):\n",
    "        \"\"\"Carrega √≠ndices salvos\"\"\"\n",
    "        input_path = Path(input_dir)\n",
    "        \n",
    "        with open(input_path / \"metadata.pkl\", 'rb') as f:\n",
    "            metadata = pickle.load(f)\n",
    "        \n",
    "        self.dataframe = pd.read_parquet(input_path / \"addresses.parquet\")\n",
    "        \n",
    "        for field in metadata['fields']:\n",
    "            self.indices[field] = faiss.read_index(str(input_path / f\"{field}_index.faiss\"))\n",
    "        \n",
    "        print(f\"üìÇ Carregados: {len(self.dataframe):,} registros\")\n",
    "        return self.indices, self.dataframe\n",
    "\n",
    "print(\"‚úÖ IndexBuilderGPU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2bdddb55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ SearchEngine\n"
     ]
    }
   ],
   "source": [
    "class SearchEngine:\n",
    "    \"\"\"\n",
    "    Motor de busca vetorial com filtragem por UF.\n",
    "    \n",
    "    - Busca com search_k aumentado quando UF fornecido\n",
    "    - Filtra resultados ap√≥s busca vetorial\n",
    "    - Convers√£o correta de dist√¢ncia L2¬≤ para similaridade cosseno\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, embedding_service: EmbeddingServiceGPU, indices: Dict[str, faiss.Index], dataframe: pd.DataFrame):\n",
    "        self.embedding_service = embedding_service\n",
    "        self.indices = indices\n",
    "        self.dataframe = dataframe\n",
    "        \n",
    "        # Pesos din√¢micos\n",
    "        self.base_weights = {\n",
    "            'with_cep': {'cep': 0.30, 'logradouro': 0.40, 'bairro': 0.20, 'cidade': 0.10},\n",
    "            'without_cep': {'logradouro': 0.55, 'bairro': 0.25, 'cidade': 0.20}\n",
    "        }\n",
    "        \n",
    "        self.confidence_threshold = 0.8\n",
    "        \n",
    "        # Criar mapeamento de √≠ndices por UF\n",
    "        self.uf_to_indices = {}\n",
    "        for uf in self.dataframe['uf'].unique():\n",
    "            self.uf_to_indices[uf] = np.where(self.dataframe['uf'] == uf)[0]\n",
    "        \n",
    "        print(f\"‚úÖ SearchEngine pronto ({len(self.uf_to_indices)} UFs indexadas)\")\n",
    "    \n",
    "    def _get_dynamic_weights(self, query: Dict[str, str]) -> Dict[str, float]:\n",
    "        \"\"\"Calcula pesos din√¢micos baseado nos campos dispon√≠veis\"\"\"\n",
    "        has_cep = bool(query.get('cep'))\n",
    "        weights = self.base_weights['with_cep' if has_cep else 'without_cep'].copy()\n",
    "        \n",
    "        available_fields = [f for f in ['logradouro', 'bairro', 'cidade'] if query.get(f)]\n",
    "        filtered_weights = {k: v for k, v in weights.items() if k in available_fields or k == 'cep'}\n",
    "        \n",
    "        total = sum(filtered_weights.values())\n",
    "        return {k: v / total for k, v in filtered_weights.items()} if total > 0 else filtered_weights\n",
    "    \n",
    "    def _calculate_field_similarity(self, field: str, query_embedding: np.ndarray, uf: str = None, top_k: int = 100) -> tuple:\n",
    "        \"\"\"\n",
    "        Busca vetorial com filtragem por UF.\n",
    "        Usa search_k maior quando UF fornecido para compensar filtro posterior.\n",
    "        \"\"\"\n",
    "        index = self.indices[field]\n",
    "        query_embedding = query_embedding.reshape(1, -1).astype(np.float32)\n",
    "        \n",
    "        # Se UF fornecido, busca mais candidatos para compensar filtro\n",
    "        search_k_adjusted = top_k * 5 if uf and uf in self.uf_to_indices else top_k\n",
    "        \n",
    "        distances, indices = index.search(query_embedding, search_k_adjusted)\n",
    "        \n",
    "        # Converter dist√¢ncias L2¬≤ para similaridade cosseno\n",
    "        # Para vetores normalizados: cos(a,b) = 1 - (||a-b||¬≤ / 2)\n",
    "        similarities = np.clip(1.0 - (distances[0] / 2.0), 0.0, 1.0)\n",
    "        \n",
    "        # Match perfeito\n",
    "        similarities[distances[0] < 1e-6] = 1.0\n",
    "        \n",
    "        return similarities, indices[0]\n",
    "    \n",
    "    def _calculate_cep_match(self, query_cep: str, db_cep: str) -> float:\n",
    "        \"\"\"Calcula similaridade de CEP\"\"\"\n",
    "        if not query_cep or not db_cep:\n",
    "            return 0.0\n",
    "        \n",
    "        query_clean = query_cep.replace('-', '').replace('.', '')\n",
    "        db_clean = db_cep.replace('-', '').replace('.', '')\n",
    "        \n",
    "        if query_clean == db_clean:\n",
    "            return 1.0\n",
    "        \n",
    "        if len(query_clean) >= 5 and len(db_clean) >= 5 and query_clean[:5] == db_clean[:5]:\n",
    "            return 0.5\n",
    "        \n",
    "        return 0.0\n",
    "    \n",
    "    def search(self, query: Dict[str, str], top_k: int = 5, search_k: int = 100, verbose: bool = False) -> str:\n",
    "        \"\"\"\n",
    "        Busca endere√ßos com filtragem precisa por UF.\n",
    "        \n",
    "        Args:\n",
    "            query: Dicion√°rio com campos (logradouro, bairro, cidade, uf, cep)\n",
    "            top_k: N√∫mero de resultados a retornar\n",
    "            search_k: Candidatos por campo (mant√©m 100 - preciso)\n",
    "            verbose: Se True, imprime detalhes da busca\n",
    "        \n",
    "        Returns:\n",
    "            JSON com resultados ordenados por score\n",
    "        \"\"\"\n",
    "        weights = self._get_dynamic_weights(query)\n",
    "        query_embeddings = self.embedding_service.embed_address_fields(query)\n",
    "        query_uf = query.get('uf')\n",
    "        \n",
    "        if verbose and query_uf:\n",
    "            print(f\"üîç Filtrando por UF={query_uf} (search_k ajustado)\")\n",
    "        \n",
    "        candidate_scores = {}\n",
    "        field_scores_map = {}\n",
    "        \n",
    "        # Busca vetorial por campo\n",
    "        for field in ['logradouro', 'bairro', 'cidade']:\n",
    "            if not query.get(field):\n",
    "                continue\n",
    "            \n",
    "            similarities, indices = self._calculate_field_similarity(\n",
    "                field, query_embeddings[field], query_uf, search_k\n",
    "            )\n",
    "            \n",
    "            weight = weights.get(field, 0.0)\n",
    "            \n",
    "            for idx, sim in zip(indices, similarities):\n",
    "                if idx == -1:  # Slot vazio do FAISS\n",
    "                    continue\n",
    "                \n",
    "                # Filtrar por UF se fornecido\n",
    "                if query_uf and self.dataframe.iloc[idx]['uf'] != query_uf:\n",
    "                    continue\n",
    "                \n",
    "                if idx not in candidate_scores:\n",
    "                    candidate_scores[idx] = 0.0\n",
    "                    field_scores_map[idx] = {}\n",
    "                \n",
    "                candidate_scores[idx] += weight * sim\n",
    "                field_scores_map[idx][field] = float(sim)\n",
    "        \n",
    "        # CEP matching\n",
    "        if query.get('cep'):\n",
    "            cep_weight = weights.get('cep', 0.0)\n",
    "            for idx in candidate_scores.keys():\n",
    "                db_cep = self.dataframe.iloc[idx]['cep']\n",
    "                cep_score = self._calculate_cep_match(query.get('cep'), db_cep)\n",
    "                candidate_scores[idx] += cep_weight * cep_score\n",
    "                field_scores_map[idx]['cep'] = cep_score\n",
    "        \n",
    "        # Ordenar e formatar resultados\n",
    "        sorted_candidates = sorted(candidate_scores.items(), key=lambda x: x[1], reverse=True)[:top_k]\n",
    "        \n",
    "        results = []\n",
    "        for idx, score in sorted_candidates:\n",
    "            row = self.dataframe.iloc[idx]\n",
    "            \n",
    "            confidence = \"high\" if score >= self.confidence_threshold else \"medium\" if score >= 0.6 else \"low\"\n",
    "            \n",
    "            results.append({\n",
    "                \"address\": {\n",
    "                    \"logradouro\": row['logradouro'],\n",
    "                    \"bairro\": row['bairro'],\n",
    "                    \"cidade\": row['cidade'],\n",
    "                    \"uf\": row['uf'],\n",
    "                    \"cep\": row['cep']\n",
    "                },\n",
    "                \"score\": float(score),\n",
    "                \"confidence\": confidence,\n",
    "                \"field_scores\": field_scores_map.get(idx, {})\n",
    "            })\n",
    "        \n",
    "        return json.dumps({\n",
    "            \"results\": results,\n",
    "            \"query\": query,\n",
    "            \"total_found\": len(results),\n",
    "            \"weights_used\": weights\n",
    "        }, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(\"‚úÖ SearchEngine\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0m0qw0ic92xa",
   "source": "### üî• Solu√ß√£o 1: SearchEngine com Reranking (MELHOR RESULTADO)",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "ffh34tx1q9k",
   "source": "class SearchEngineWithReranking(SearchEngine):\n    \"\"\"\n    Motor de busca com RERANKING baseado em interse√ß√£o de candidatos.\n    \n    Estrat√©gia:\n    1. Busca mais candidatos por campo (search_k_large)\n    2. Identifica candidatos que aparecem em M√öLTIPLOS campos\n    3. D√° boost para candidatos na interse√ß√£o\n    4. Retorna resultados com score ajustado\n    \n    BENEF√çCIO: Resolve o problema de \"candidatos diferentes por campo\"\n    \"\"\"\n    \n    def search_with_reranking(\n        self, \n        query: Dict[str, str], \n        top_k: int = 5, \n        search_k: int = 500,  # Aumentado!\n        intersection_boost: float = 0.2,  # Boost de 20% para interse√ß√£o\n        verbose: bool = False\n    ) -> str:\n        \"\"\"\n        Busca com reranking baseado em interse√ß√£o.\n        \n        Args:\n            query: Campos do endere√ßo\n            top_k: Resultados finais\n            search_k: Candidatos por campo (mais = melhor)\n            intersection_boost: Boost para candidatos em m√∫ltiplos campos\n            verbose: Debug\n        \"\"\"\n        weights = self._get_dynamic_weights(query)\n        query_embeddings = self.embedding_service.embed_address_fields(query)\n        query_uf = query.get('uf')\n        \n        # ETAPA 1: Buscar candidatos por campo\n        field_candidates = {}  # field -> {idx: similarity}\n        \n        for field in ['logradouro', 'bairro', 'cidade']:\n            if not query.get(field):\n                continue\n            \n            similarities, indices = self._calculate_field_similarity(\n                field, query_embeddings[field], query_uf, search_k\n            )\n            \n            field_candidates[field] = {}\n            for idx, sim in zip(indices, similarities):\n                if idx == -1:\n                    continue\n                if query_uf and self.dataframe.iloc[idx]['uf'] != query_uf:\n                    continue\n                field_candidates[field][idx] = float(sim)\n        \n        # ETAPA 2: Identificar interse√ß√µes\n        all_candidate_ids = set()\n        for candidates in field_candidates.values():\n            all_candidate_ids.update(candidates.keys())\n        \n        # Contar em quantos campos cada candidato aparece\n        candidate_field_count = {}\n        for idx in all_candidate_ids:\n            count = sum(1 for field_cands in field_candidates.values() if idx in field_cands)\n            candidate_field_count[idx] = count\n        \n        if verbose:\n            num_fields_searched = len(field_candidates)\n            intersection_ids = [idx for idx, count in candidate_field_count.items() if count == num_fields_searched]\n            print(f\"üîç Candidatos em TODOS os {num_fields_searched} campos: {len(intersection_ids)}\")\n            print(f\"üîç Total de candidatos √∫nicos: {len(all_candidate_ids)}\")\n        \n        # ETAPA 3: Calcular scores com boost para interse√ß√£o\n        candidate_scores = {}\n        field_scores_map = {}\n        \n        for idx in all_candidate_ids:\n            score = 0.0\n            field_scores_map[idx] = {}\n            \n            # Score vetorial ponderado\n            for field, candidates in field_candidates.items():\n                if idx in candidates:\n                    sim = candidates[idx]\n                    weight = weights.get(field, 0.0)\n                    score += weight * sim\n                    field_scores_map[idx][field] = sim\n            \n            # Boost para candidatos em m√∫ltiplos campos\n            num_fields = len(field_candidates)\n            if candidate_field_count[idx] == num_fields:\n                # Candidato aparece em TODOS os campos\n                score = score * (1 + intersection_boost)\n                if verbose and len(candidate_scores) < 3:\n                    print(f\"   Boost aplicado ao candidato {idx}: +{intersection_boost*100:.0f}%\")\n            elif candidate_field_count[idx] >= num_fields - 1:\n                # Candidato aparece em quase todos os campos\n                score = score * (1 + intersection_boost * 0.5)\n            \n            candidate_scores[idx] = score\n        \n        # CEP matching (sem boost, apenas score)\n        if query.get('cep'):\n            cep_weight = weights.get('cep', 0.0)\n            for idx in candidate_scores.keys():\n                db_cep = self.dataframe.iloc[idx]['cep']\n                cep_score = self._calculate_cep_match(query.get('cep'), db_cep)\n                candidate_scores[idx] += cep_weight * cep_score\n                field_scores_map[idx]['cep'] = cep_score\n        \n        # ETAPA 4: Ordenar e formatar\n        sorted_candidates = sorted(candidate_scores.items(), key=lambda x: x[1], reverse=True)[:top_k]\n        \n        results = []\n        for idx, score in sorted_candidates:\n            row = self.dataframe.iloc[idx]\n            \n            # Normalizar score se passou de 1.0 por causa do boost\n            display_score = min(score, 1.0)\n            \n            confidence = \"high\" if display_score >= self.confidence_threshold else \"medium\" if display_score >= 0.6 else \"low\"\n            \n            results.append({\n                \"address\": {\n                    \"logradouro\": row['logradouro'],\n                    \"bairro\": row['bairro'],\n                    \"cidade\": row['cidade'],\n                    \"uf\": row['uf'],\n                    \"cep\": row['cep']\n                },\n                \"score\": float(display_score),\n                \"raw_score\": float(score),  # Score antes da normaliza√ß√£o\n                \"confidence\": confidence,\n                \"field_scores\": field_scores_map.get(idx, {}),\n                \"num_fields_matched\": candidate_field_count.get(idx, 0)\n            })\n        \n        return json.dumps({\n            \"results\": results,\n            \"query\": query,\n            \"total_found\": len(results),\n            \"weights_used\": weights,\n            \"reranking_config\": {\n                \"search_k\": search_k,\n                \"intersection_boost\": intersection_boost\n            }\n        }, ensure_ascii=False, indent=2)\n\nprint(\"‚úÖ SearchEngineWithReranking\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "2fcf5115",
   "metadata": {},
   "source": [
    "## 3. Carregar DNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8762801",
   "metadata": {},
   "outputs": [
    {
     "ename": "ArrowInvalid",
     "evalue": "No match for FieldRef.Name(logradouro_completo) in id: int64\ncep: string\nlogradouro: string\nbairro: string\ncidade: string\nuf: string\ntipo_teste: string\ncep_norm: string\nlogradouro_norm: string\nbairro_norm: string\ncidade_norm: string\nuf_norm: string\n__fragment_index: int32\n__batch_index: int32\n__last_in_fragment: bool\n__filename: string",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mArrowInvalid\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Carregar dataset\u001b[39;00m\n\u001b[32m      2\u001b[39m dne_path = Path(\u001b[33m'\u001b[39m\u001b[33m../data/dne_normalizado.parquet\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m df_dne = \u001b[43mpd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread_parquet\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdne_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mlogradouro_completo\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mbairro_completo\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mcidade_completo\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43muf\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mcep\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m      9\u001b[39m df_dne = df_dne.rename(columns={\n\u001b[32m     10\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mlogradouro_completo\u001b[39m\u001b[33m'\u001b[39m: \u001b[33m'\u001b[39m\u001b[33mlogradouro\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m     11\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mbairro_completo\u001b[39m\u001b[33m'\u001b[39m: \u001b[33m'\u001b[39m\u001b[33mbairro\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m     12\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mcidade_completo\u001b[39m\u001b[33m'\u001b[39m: \u001b[33m'\u001b[39m\u001b[33mcidade\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m     13\u001b[39m })\n\u001b[32m     15\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m‚úÖ Dataset: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(df_dne)\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m,\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m registros\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\rik_d\\OneDrive\\Documentos\\Projetos\\Backend\\IA\\genia\\venv\\Lib\\site-packages\\pandas\\io\\parquet.py:669\u001b[39m, in \u001b[36mread_parquet\u001b[39m\u001b[34m(path, engine, columns, storage_options, use_nullable_dtypes, dtype_backend, filesystem, filters, **kwargs)\u001b[39m\n\u001b[32m    666\u001b[39m     use_nullable_dtypes = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m    667\u001b[39m check_dtype_backend(dtype_backend)\n\u001b[32m--> \u001b[39m\u001b[32m669\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mimpl\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    670\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    671\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    672\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfilters\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfilters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    673\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    674\u001b[39m \u001b[43m    \u001b[49m\u001b[43muse_nullable_dtypes\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_nullable_dtypes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    675\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdtype_backend\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdtype_backend\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    676\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfilesystem\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfilesystem\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    677\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    678\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\rik_d\\OneDrive\\Documentos\\Projetos\\Backend\\IA\\genia\\venv\\Lib\\site-packages\\pandas\\io\\parquet.py:265\u001b[39m, in \u001b[36mPyArrowImpl.read\u001b[39m\u001b[34m(self, path, columns, filters, use_nullable_dtypes, dtype_backend, storage_options, filesystem, **kwargs)\u001b[39m\n\u001b[32m    258\u001b[39m path_or_handle, handles, filesystem = _get_path_or_handle(\n\u001b[32m    259\u001b[39m     path,\n\u001b[32m    260\u001b[39m     filesystem,\n\u001b[32m    261\u001b[39m     storage_options=storage_options,\n\u001b[32m    262\u001b[39m     mode=\u001b[33m\"\u001b[39m\u001b[33mrb\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    263\u001b[39m )\n\u001b[32m    264\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m265\u001b[39m     pa_table = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mapi\u001b[49m\u001b[43m.\u001b[49m\u001b[43mparquet\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread_table\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    266\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpath_or_handle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    267\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    268\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfilesystem\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfilesystem\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    269\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfilters\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfilters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    270\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    271\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    273\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m catch_warnings():\n\u001b[32m    274\u001b[39m         filterwarnings(\n\u001b[32m    275\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mignore\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    276\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mmake_block is deprecated\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    277\u001b[39m             \u001b[38;5;167;01mDeprecationWarning\u001b[39;00m,\n\u001b[32m    278\u001b[39m         )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\rik_d\\OneDrive\\Documentos\\Projetos\\Backend\\IA\\genia\\venv\\Lib\\site-packages\\pyarrow\\parquet\\core.py:1898\u001b[39m, in \u001b[36mread_table\u001b[39m\u001b[34m(source, columns, use_threads, schema, use_pandas_metadata, read_dictionary, binary_type, list_type, memory_map, buffer_size, partitioning, filesystem, filters, ignore_prefixes, pre_buffer, coerce_int96_timestamp_unit, decryption_properties, thrift_string_size_limit, thrift_container_size_limit, page_checksum_verification, arrow_extensions_enabled)\u001b[39m\n\u001b[32m   1884\u001b[39m     \u001b[38;5;66;03m# TODO test that source is not a directory or a list\u001b[39;00m\n\u001b[32m   1885\u001b[39m     dataset = ParquetFile(\n\u001b[32m   1886\u001b[39m         source, read_dictionary=read_dictionary,\n\u001b[32m   1887\u001b[39m         binary_type=binary_type,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1895\u001b[39m         page_checksum_verification=page_checksum_verification,\n\u001b[32m   1896\u001b[39m     )\n\u001b[32m-> \u001b[39m\u001b[32m1898\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdataset\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muse_threads\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_threads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1899\u001b[39m \u001b[43m                    \u001b[49m\u001b[43muse_pandas_metadata\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_pandas_metadata\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\rik_d\\OneDrive\\Documentos\\Projetos\\Backend\\IA\\genia\\venv\\Lib\\site-packages\\pyarrow\\parquet\\core.py:1538\u001b[39m, in \u001b[36mParquetDataset.read\u001b[39m\u001b[34m(self, columns, use_threads, use_pandas_metadata)\u001b[39m\n\u001b[32m   1530\u001b[39m         index_columns = [\n\u001b[32m   1531\u001b[39m             col \u001b[38;5;28;01mfor\u001b[39;00m col \u001b[38;5;129;01min\u001b[39;00m _get_pandas_index_columns(metadata)\n\u001b[32m   1532\u001b[39m             \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(col, \u001b[38;5;28mdict\u001b[39m)\n\u001b[32m   1533\u001b[39m         ]\n\u001b[32m   1534\u001b[39m         columns = (\n\u001b[32m   1535\u001b[39m             \u001b[38;5;28mlist\u001b[39m(columns) + \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mset\u001b[39m(index_columns) - \u001b[38;5;28mset\u001b[39m(columns))\n\u001b[32m   1536\u001b[39m         )\n\u001b[32m-> \u001b[39m\u001b[32m1538\u001b[39m table = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_dataset\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto_table\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1539\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mfilter\u001b[39;49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_filter_expression\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1540\u001b[39m \u001b[43m    \u001b[49m\u001b[43muse_threads\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_threads\u001b[49m\n\u001b[32m   1541\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1543\u001b[39m \u001b[38;5;66;03m# if use_pandas_metadata, restore the pandas metadata (which gets\u001b[39;00m\n\u001b[32m   1544\u001b[39m \u001b[38;5;66;03m# lost if doing a specific `columns` selection in to_table)\u001b[39;00m\n\u001b[32m   1545\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m use_pandas_metadata:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\rik_d\\OneDrive\\Documentos\\Projetos\\Backend\\IA\\genia\\venv\\Lib\\site-packages\\pyarrow\\_dataset.pyx:579\u001b[39m, in \u001b[36mpyarrow._dataset.Dataset.to_table\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\rik_d\\OneDrive\\Documentos\\Projetos\\Backend\\IA\\genia\\venv\\Lib\\site-packages\\pyarrow\\_dataset.pyx:415\u001b[39m, in \u001b[36mpyarrow._dataset.Dataset.scanner\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\rik_d\\OneDrive\\Documentos\\Projetos\\Backend\\IA\\genia\\venv\\Lib\\site-packages\\pyarrow\\_dataset.pyx:3674\u001b[39m, in \u001b[36mpyarrow._dataset.Scanner.from_dataset\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\rik_d\\OneDrive\\Documentos\\Projetos\\Backend\\IA\\genia\\venv\\Lib\\site-packages\\pyarrow\\_dataset.pyx:3587\u001b[39m, in \u001b[36mpyarrow._dataset.Scanner._make_scan_options\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\rik_d\\OneDrive\\Documentos\\Projetos\\Backend\\IA\\genia\\venv\\Lib\\site-packages\\pyarrow\\_dataset.pyx:3537\u001b[39m, in \u001b[36mpyarrow._dataset._populate_builder\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\rik_d\\OneDrive\\Documentos\\Projetos\\Backend\\IA\\genia\\venv\\Lib\\site-packages\\pyarrow\\error.pxi:92\u001b[39m, in \u001b[36mpyarrow.lib.check_status\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[31mArrowInvalid\u001b[39m: No match for FieldRef.Name(logradouro_completo) in id: int64\ncep: string\nlogradouro: string\nbairro: string\ncidade: string\nuf: string\ntipo_teste: string\ncep_norm: string\nlogradouro_norm: string\nbairro_norm: string\ncidade_norm: string\nuf_norm: string\n__fragment_index: int32\n__batch_index: int32\n__last_in_fragment: bool\n__filename: string"
     ]
    }
   ],
   "source": [
    "# Carregar dataset\n",
    "dne_path = Path('../data/dne_normalizado.parquet')\n",
    "\n",
    "df_dne = pd.read_parquet(\n",
    "    dne_path,\n",
    "    columns=['logradouro', 'bairro', 'cidade', 'uf', 'cep']\n",
    ")\n",
    "\n",
    "print(f\"‚úÖ Dataset: {len(df_dne):,} registros\")\n",
    "print(f\"\\nüìä Top 5 UFs:\")\n",
    "print(df_dne['uf'].value_counts().head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3377b9ed",
   "metadata": {},
   "source": [
    "## 4. Construir √çndices (executar UMA VEZ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2524ee46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inicializar servi√ßo de embeddings\n",
    "embedding_service = EmbeddingServiceGPU(\n",
    "    model_name=\"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\",\n",
    "    use_fp16=True\n",
    ")\n",
    "\n",
    "# Construir √≠ndices\n",
    "index_builder = IndexBuilderGPU(embedding_service)\n",
    "indices = index_builder.build_indices(df_dne, M=32, efSearch=32)\n",
    "\n",
    "# Salvar para reutiliza√ß√£o\n",
    "index_builder.save_indices('../data/indices_gpu_t4')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "587c42ce",
   "metadata": {},
   "source": [
    "## 5. Carregar √çndices (sempre usar ap√≥s primeira constru√ß√£o)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b714c0c6",
   "metadata": {},
   "outputs": [],
   "source": "# Carregar modelo\nembedding_service = EmbeddingServiceGPU(\n    model_name=\"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\",\n    use_fp16=True\n)\n\n# Carregar √≠ndices salvos (corrigido caminho)\nindex_builder = IndexBuilderGPU(embedding_service)\nindices, df_dne = index_builder.load_indices('../data/indices')\n\n# Inicializar motor de busca\nsearch_engine = SearchEngine(embedding_service, indices, df_dne)\n\nprint(f\"\\nüöÄ Sistema pronto!\")"
  },
  {
   "cell_type": "code",
   "id": "8i8l0ad7saq",
   "source": "# Criar ambos os motores de busca para compara√ß√£o\nsearch_engine_original = SearchEngine(embedding_service, indices, df_dne)\nsearch_engine_reranking = SearchEngineWithReranking(embedding_service, indices, df_dne)\n\nprint(f\"\\nüöÄ Ambos os sistemas prontos para compara√ß√£o!\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "czx9h2gsv76",
   "source": "## ‚ö° Compara√ß√£o: Original vs Reranking",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "0wkdsy4lvs1a",
   "source": "\"\"\"\nüÜö COMPARA√á√ÉO DIRETA: Original vs Reranking\n\nTeste com 10 endere√ßos aleat√≥rios da base para ver a diferen√ßa de score.\n\"\"\"\n\nprint(\"=\"*80)\nprint(\"üÜö TESTE COMPARATIVO: Original vs Reranking\")\nprint(\"=\"*80)\n\nn_tests = 10\ncomparison_results = []\n\nfor test_num in range(n_tests):\n    print(f\"\\n{'‚îÄ'*80}\")\n    print(f\"üìç Teste {test_num + 1}/{n_tests}\")\n    print(f\"{'‚îÄ'*80}\")\n    \n    # Pegar endere√ßo aleat√≥rio\n    sample_idx = np.random.choice(len(df_dne))\n    ground_truth = df_dne.iloc[sample_idx]\n    \n    query = {\n        \"logradouro\": ground_truth['logradouro'],\n        \"bairro\": ground_truth['bairro'],\n        \"cidade\": ground_truth['cidade'],\n        \"uf\": ground_truth['uf'],\n        \"cep\": ground_truth['cep']\n    }\n    \n    print(f\"   Query: {ground_truth['logradouro'][:40]}, {ground_truth['cidade']}/{ground_truth['uf']}\")\n    \n    # Busca original\n    result_orig = json.loads(search_engine_original.search(query, top_k=1, search_k=100))\n    top_orig = result_orig['results'][0]\n    is_exact_orig = (\n        top_orig['address']['logradouro'] == ground_truth['logradouro'] and\n        top_orig['address']['bairro'] == ground_truth['bairro'] and\n        top_orig['address']['cidade'] == ground_truth['cidade'] and\n        top_orig['address']['cep'] == ground_truth['cep']\n    )\n    \n    # Busca com reranking\n    result_rerank = json.loads(search_engine_reranking.search_with_reranking(query, top_k=1, search_k=500))\n    top_rerank = result_rerank['results'][0]\n    is_exact_rerank = (\n        top_rerank['address']['logradouro'] == ground_truth['logradouro'] and\n        top_rerank['address']['bairro'] == ground_truth['bairro'] and\n        top_rerank['address']['cidade'] == ground_truth['cidade'] and\n        top_rerank['address']['cep'] == ground_truth['cep']\n    )\n    \n    # Comparar\n    orig_score = top_orig['score']\n    rerank_score = top_rerank['score']\n    improvement = rerank_score - orig_score\n    \n    print(f\"   Original:  {orig_score:.4f} {'‚úÖ' if is_exact_orig else '‚ùå'}\")\n    print(f\"   Reranking: {rerank_score:.4f} {'‚úÖ' if is_exact_rerank else '‚ùå'}\")\n    \n    if improvement > 0.01:\n        print(f\"   üìà Melhoria: +{improvement:.4f} ({improvement*100:.1f}%)\")\n    elif improvement < -0.01:\n        print(f\"   üìâ Piora: {improvement:.4f} ({improvement*100:.1f}%)\")\n    else:\n        print(f\"   ‚û°Ô∏è  Sem mudan√ßa significativa\")\n    \n    comparison_results.append({\n        'test_num': test_num + 1,\n        'orig_score': orig_score,\n        'rerank_score': rerank_score,\n        'improvement': improvement,\n        'is_exact_orig': is_exact_orig,\n        'is_exact_rerank': is_exact_rerank\n    })\n\n# Resumo estat√≠stico\nprint(f\"\\n{'='*80}\")\nprint(f\"üìä RESUMO ESTAT√çSTICO\")\nprint(f\"{'='*80}\")\n\norig_scores = [r['orig_score'] for r in comparison_results]\nrerank_scores = [r['rerank_score'] for r in comparison_results]\nimprovements = [r['improvement'] for r in comparison_results]\n\nexact_orig = sum(r['is_exact_orig'] for r in comparison_results)\nexact_rerank = sum(r['is_exact_rerank'] for r in comparison_results)\n\nprint(f\"\\nScores M√©dios:\")\nprint(f\"   Original:  {np.mean(orig_scores):.4f} ({np.mean(orig_scores)*100:.2f}%)\")\nprint(f\"   Reranking: {np.mean(rerank_scores):.4f} ({np.mean(rerank_scores)*100:.2f}%)\")\nprint(f\"   Melhoria:  +{np.mean(improvements):.4f} ({np.mean(improvements)*100:.2f}%)\")\n\nprint(f\"\\nMatches Exatos:\")\nprint(f\"   Original:  {exact_orig}/{n_tests} ({exact_orig/n_tests*100:.1f}%)\")\nprint(f\"   Reranking: {exact_rerank}/{n_tests} ({exact_rerank/n_tests*100:.1f}%)\")\n\n# Contabilizar melhorias\nnum_better = sum(1 for r in comparison_results if r['improvement'] > 0.01)\nnum_worse = sum(1 for r in comparison_results if r['improvement'] < -0.01)\nnum_same = n_tests - num_better - num_worse\n\nprint(f\"\\nDesempenho Relativo:\")\nprint(f\"   Melhor:    {num_better}/{n_tests} ({num_better/n_tests*100:.1f}%)\")\nprint(f\"   Pior:      {num_worse}/{n_tests} ({num_worse/n_tests*100:.1f}%)\")\nprint(f\"   Sem mudan√ßa: {num_same}/{n_tests} ({num_same/n_tests*100:.1f}%)\")\n\nprint(f\"\\n{'='*80}\")\nif np.mean(rerank_scores) > np.mean(orig_scores) + 0.02:\n    print(f\"‚úÖ CONCLUS√ÉO: Reranking melhora significativamente a precis√£o!\")\n    print(f\"   Recomenda√ß√£o: USAR search_with_reranking() em produ√ß√£o\")\nelif np.mean(rerank_scores) > np.mean(orig_scores):\n    print(f\"‚úÖ CONCLUS√ÉO: Reranking melhora ligeiramente a precis√£o\")\n    print(f\"   Recomenda√ß√£o: CONSIDERAR usar reranking (trade-off: +50-100ms)\")\nelse:\n    print(f\"‚ö†Ô∏è  CONCLUS√ÉO: Reranking n√£o trouxe melhoria significativa\")\n    print(f\"   Poss√≠vel que search_k j√° esteja otimizado ou base tenha poucos duplicados\")\n\nprint(f\"=\"*80)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "cniczte9q0i",
   "source": "\"\"\"\nüß™ TESTE SINT√âTICO SIMPLIFICADO: Por que n√£o atinge 100%?\n\nEste teste pega um endere√ßo real da base e busca com os MESMOS dados.\nObjetivo: Entender onde est√° a perda de score mesmo com dados perfeitos.\n\"\"\"\n\nprint(\"=\"*80)\nprint(\"üß™ TESTE SINT√âTICO: Valida√ß√£o de Score M√°ximo\")\nprint(\"=\"*80)\n\n# 1. Selecionar 5 endere√ßos aleat√≥rios para teste\nn_tests = 5\ntest_results = []\n\nfor test_num in range(n_tests):\n    print(f\"\\n{'='*80}\")\n    print(f\"üìç TESTE {test_num + 1}/{n_tests}\")\n    print(f\"{'='*80}\")\n    \n    # Selecionar endere√ßo aleat√≥rio\n    sample_idx = np.random.choice(len(df_dne))\n    ground_truth = df_dne.iloc[sample_idx]\n    \n    print(f\"\\nüìå Ground Truth (√≠ndice {sample_idx}):\")\n    print(f\"   {ground_truth['logradouro']}\")\n    print(f\"   {ground_truth['bairro']} - {ground_truth['cidade']}/{ground_truth['uf']}\")\n    print(f\"   CEP: {ground_truth['cep']}\")\n    \n    # Criar query EXATA\n    query = {\n        \"logradouro\": ground_truth['logradouro'],\n        \"bairro\": ground_truth['bairro'],\n        \"cidade\": ground_truth['cidade'],\n        \"uf\": ground_truth['uf'],\n        \"cep\": ground_truth['cep']\n    }\n    \n    # Buscar\n    result_json = search_engine.search(query, top_k=5, search_k=100)\n    result = json.loads(result_json)\n    \n    # Analisar top result\n    top = result['results'][0]\n    top_addr = top['address']\n    \n    # Verificar se √© match exato\n    is_exact = (\n        top_addr['logradouro'] == ground_truth['logradouro'] and\n        top_addr['bairro'] == ground_truth['bairro'] and\n        top_addr['cidade'] == ground_truth['cidade'] and\n        top_addr['uf'] == ground_truth['uf'] and\n        top_addr['cep'] == ground_truth['cep']\n    )\n    \n    print(f\"\\nüèÜ Top 1 Result:\")\n    print(f\"   Score: {top['score']:.4f} ({top['score']*100:.2f}%)\")\n    print(f\"   Match exato: {'‚úÖ SIM' if is_exact else '‚ùå N√ÉO'}\")\n    \n    if is_exact:\n        print(f\"\\n   Field Scores:\")\n        for field in ['logradouro', 'bairro', 'cidade', 'cep']:\n            if field in top['field_scores']:\n                score = top['field_scores'][field]\n                weight = result['weights_used'][field]\n                contrib = score * weight\n                status = \"‚úÖ\" if score >= 0.99 else \"‚ö†Ô∏è\"\n                print(f\"      {status} {field:12s}: {score:.4f} √ó {weight:.2f} = {contrib:.4f}\")\n    else:\n        print(f\"   ‚ùå PROBLEMA: Retornou endere√ßo diferente!\")\n        print(f\"      {top_addr['logradouro']}, {top_addr['bairro']}\")\n        print(f\"      {top_addr['cidade']}/{top_addr['uf']} - CEP: {top_addr['cep']}\")\n    \n    test_results.append({\n        'test_num': test_num + 1,\n        'score': top['score'],\n        'is_exact_match': is_exact,\n        'ground_truth_idx': sample_idx\n    })\n\n# Resumo final\nprint(f\"\\n{'='*80}\")\nprint(f\"üìä RESUMO DOS TESTES\")\nprint(f\"{'='*80}\")\n\nexact_matches = sum(1 for r in test_results if r['is_exact_match'])\navg_score = np.mean([r['score'] for r in test_results if r['is_exact_match']])\n\nprint(f\"\\nTotal de testes: {n_tests}\")\nprint(f\"Matches exatos: {exact_matches}/{n_tests} ({exact_matches/n_tests*100:.1f}%)\")\n\nif exact_matches > 0:\n    print(f\"Score m√©dio (matches exatos): {avg_score:.4f} ({avg_score*100:.2f}%)\")\n    \n    if avg_score >= 0.99:\n        print(f\"\\n‚úÖ CONCLUS√ÉO: Sistema funcionando corretamente!\")\n        print(f\"   Scores ~100% para dados id√™nticos (pequena perda por imprecis√£o num√©rica)\")\n    elif avg_score >= 0.95:\n        print(f\"\\n‚ö†Ô∏è  CONCLUS√ÉO: Score bom mas abaixo de 100%\")\n        print(f\"   Poss√≠veis causas:\")\n        print(f\"   - Embeddings vetoriais t√™m imprecis√£o num√©rica\")\n        print(f\"   - HNSW √© aproximado (n√£o retorna sempre o vizinho mais pr√≥ximo)\")\n    else:\n        print(f\"\\n‚ùå CONCLUS√ÉO: Score muito abaixo do esperado\")\n        print(f\"   Problema na busca vetorial ou nos pesos\")\nelse:\n    print(f\"\\n‚ùå CONCLUS√ÉO CR√çTICA: Nenhum teste retornou o pr√≥prio endere√ßo!\")\n    print(f\"   Problema grave no sistema de busca\")\n\nprint(f\"{'='*80}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "u6jx42lvonq",
   "source": "\"\"\"\nüî¨ AN√ÅLISE DETALHADA: Por campo individual\n\nPara diagnosticar onde exatamente est√° a perda de score, vamos:\n1. Testar cada campo separadamente\n2. Ver se o ground truth aparece como top 1 em cada campo\n3. Verificar a similaridade vetorial de cada campo\n\"\"\"\n\nprint(\"=\"*80)\nprint(\"üî¨ AN√ÅLISE CAMPO POR CAMPO\")\nprint(\"=\"*80)\n\n# Usar um endere√ßo espec√≠fico para an√°lise detalhada\nsample_idx = np.random.choice(len(df_dne))\nground_truth = df_dne.iloc[sample_idx]\n\nprint(f\"\\nüìç Endere√ßo Selecionado (√≠ndice {sample_idx}):\")\nprint(f\"   Logradouro: {ground_truth['logradouro']}\")\nprint(f\"   Bairro:     {ground_truth['bairro']}\")\nprint(f\"   Cidade:     {ground_truth['cidade']}\")\nprint(f\"   UF:         {ground_truth['uf']}\")\nprint(f\"   CEP:        {ground_truth['cep']}\")\n\nfield_analysis = {}\n\n# Analisar cada campo\nfor field in ['logradouro', 'bairro', 'cidade']:\n    print(f\"\\n{'='*80}\")\n    print(f\"üìå An√°lise do campo: {field.upper()}\")\n    print(f\"{'='*80}\")\n    \n    query_text = ground_truth[field]\n    print(f\"   Query: '{query_text}'\")\n    \n    # Gerar embedding\n    query_emb = search_engine.embedding_service.embed_text(query_text)\n    \n    # Buscar (aumentar search_k para garantir que encontre)\n    similarities, indices = search_engine._calculate_field_similarity(\n        field, query_emb, ground_truth['uf'], top_k=1000\n    )\n    \n    # Filtrar v√°lidos e por UF\n    valid_results = []\n    for idx, sim in zip(indices, similarities):\n        if idx == -1:\n            continue\n        if df_dne.iloc[idx]['uf'] == ground_truth['uf']:\n            valid_results.append((idx, sim))\n    \n    # Encontrar ground truth\n    gt_found = False\n    gt_rank = -1\n    gt_sim = 0.0\n    \n    for rank, (idx, sim) in enumerate(valid_results[:100], 1):\n        if idx == sample_idx:\n            gt_found = True\n            gt_rank = rank\n            gt_sim = sim\n            break\n    \n    # Resultado da an√°lise\n    if gt_found:\n        status = \"‚úÖ\" if gt_rank == 1 else \"‚ö†Ô∏è\"\n        print(f\"   {status} Ground truth encontrado!\")\n        print(f\"   Rank: {gt_rank}\")\n        print(f\"   Similaridade: {gt_sim:.6f}\")\n        \n        if gt_rank == 1 and gt_sim >= 0.9999:\n            quality = \"PERFEITO\"\n        elif gt_rank == 1:\n            quality = \"BOM (rank 1, similaridade n√£o perfeita)\"\n        else:\n            quality = f\"PROBLEMA (rank {gt_rank})\"\n        \n        print(f\"   Qualidade: {quality}\")\n    else:\n        print(f\"   ‚ùå Ground truth N√ÉO encontrado nos top 100!\")\n        quality = \"FALHA CR√çTICA\"\n    \n    # Top 3 para compara√ß√£o\n    print(f\"\\n   Top 3 candidatos:\")\n    for rank, (idx, sim) in enumerate(valid_results[:3], 1):\n        value = df_dne.iloc[idx][field]\n        is_gt = \" ‚Üê GROUND TRUTH\" if idx == sample_idx else \"\"\n        print(f\"      {rank}. sim={sim:.6f} | '{value[:60]}'{is_gt}\")\n    \n    field_analysis[field] = {\n        'found': gt_found,\n        'rank': gt_rank,\n        'similarity': gt_sim,\n        'quality': quality\n    }\n\n# CEP - match exato\nprint(f\"\\n{'='*80}\")\nprint(f\"üìå An√°lise do campo: CEP\")\nprint(f\"{'='*80}\")\n\nquery_cep = ground_truth['cep']\ncep_match = search_engine._calculate_cep_match(query_cep, query_cep)\nprint(f\"   Query: '{query_cep}'\")\nprint(f\"   Score: {cep_match:.4f}\")\nfield_analysis['cep'] = {\n    'found': True,\n    'rank': 1,\n    'similarity': cep_match,\n    'quality': 'PERFEITO' if cep_match == 1.0 else 'PROBLEMA'\n}\n\n# Diagn√≥stico final\nprint(f\"\\n{'='*80}\")\nprint(f\"üí° DIAGN√ìSTICO FINAL\")\nprint(f\"{'='*80}\")\n\nall_rank_1 = all(v['rank'] == 1 for v in field_analysis.values() if v['found'])\nall_found = all(v['found'] for v in field_analysis.values())\nall_high_sim = all(v['similarity'] >= 0.99 for v in field_analysis.values())\n\nprint(f\"\\nResumo por campo:\")\nfor field, analysis in field_analysis.items():\n    status = \"‚úÖ\" if analysis['quality'] in ['PERFEITO', 'BOM (rank 1, similaridade n√£o perfeita)'] else \"‚ùå\"\n    print(f\"   {status} {field:12s}: {analysis['quality']}\")\n\nprint(f\"\\n{'='*80}\")\n\nif all_rank_1 and all_high_sim:\n    print(f\"‚úÖ IDEAL: Todos os campos retornam o ground truth em rank 1\")\n    print(f\"   ‚Üí Score final deve ser ~100%\")\n    print(f\"   ‚Üí Pequenas perdas s√£o apenas por imprecis√£o num√©rica dos embeddings\")\nelif all_found and all_rank_1:\n    print(f\"‚ö†Ô∏è  BOM: Todos os campos retornam o ground truth, mas similaridade n√£o perfeita\")\n    print(f\"   ‚Üí Score final ser√° alto mas < 100%\")\n    print(f\"   ‚Üí Causa: imprecis√£o num√©rica ou normaliza√ß√£o de texto\")\nelif all_found:\n    print(f\"‚ùå PROBLEMA: Ground truth encontrado mas n√£o em rank 1 em todos os campos\")\n    print(f\"   ‚Üí Score final ser√° baixo pois cada campo retorna candidatos diferentes\")\n    print(f\"   ‚Üí Causa: textos gen√©ricos (ex: 'centro', 'rua principal') t√™m muitos matches\")\n    print(f\"   ‚Üí Solu√ß√£o: aumentar search_k ou usar √≠ndices separados por UF\")\nelse:\n    print(f\"‚ùå PROBLEMA CR√çTICO: Ground truth n√£o encontrado em algum campo\")\n    print(f\"   ‚Üí Sistema de busca vetorial tem problema grave\")\n    print(f\"   ‚Üí Verificar constru√ß√£o dos √≠ndices FAISS\")\n\nprint(f\"=\"*80)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "o662syumk3i",
   "source": "## üéØ An√°lise: Por que n√£o 100% de precis√£o?\n\n### Causas Identificadas\n\nCom base nos testes sint√©ticos acima, aqui est√£o as causas mais comuns para n√£o atingir 100% de score:\n\n#### 1Ô∏è‚É£ **Imprecis√£o Num√©rica dos Embeddings** (Score ~99-99.9%)\n- **O que √©**: Mesmo texto exato gera embeddings com pequena varia√ß√£o num√©rica\n- **Causa**: Arredondamento float32, opera√ß√µes do modelo de embedding\n- **Impacto**: Perda de 0.1-1% no score final\n- **Solu√ß√£o**: **NORMAL** - aceitar como caracter√≠stica do sistema vetorial\n\n#### 2Ô∏è‚É£ **Aproxima√ß√£o do HNSW** (Score ~95-99%)\n- **O que √©**: FAISS HNSW √© um √≠ndice aproximado, n√£o exato\n- **Causa**: Algoritmo HNSW pode n√£o retornar o vizinho EXATO mais pr√≥ximo\n- **Impacto**: Ground truth pode aparecer em rank 2-5 ao inv√©s de rank 1\n- **Solu√ß√£o**: Aumentar `efSearch` na constru√ß√£o (trade-off: mais mem√≥ria/tempo)\n\n#### 3Ô∏è‚É£ **Candidatos Diferentes por Campo** (Score ~70-95%)\n- **O que √©**: Cada campo retorna candidatos diferentes nos top-k\n- **Causa**: Nomes gen√©ricos (ex: \"Centro\", \"Rua Principal\") t√™m muitos matches similares\n- **Impacto**: Score final √© soma de partes, n√£o h√° candidato que seja top-1 em todos os campos\n- **Exemplo**:\n  - Logradouro \"Rua das Flores\" ‚Üí retorna √≠ndice 100 (sim: 0.95)\n  - Bairro \"Centro\" ‚Üí retorna √≠ndice 250 (sim: 0.92)\n  - Cidade \"S√£o Paulo\" ‚Üí retorna √≠ndice 180 (sim: 0.98)\n  - Score final = 0.95√ó0.4 + 0.92√ó0.25 + 0.98√ó0.20 = **0.804 (80.4%)**\n- **Solu√ß√£o**: \n  - Aumentar `search_k` (buscar mais candidatos por campo)\n  - Usar √≠ndices separados por UF (menor pool de candidatos)\n  - Reranking: pegar top-100 de cada campo, fazer interse√ß√£o\n\n#### 4Ô∏è‚É£ **Normaliza√ß√£o de Texto Inconsistente** (Score ~80-90%)\n- **O que √©**: Query normalizado diferente do texto indexado\n- **Causa**: Abrevia√ß√µes, acentos, caracteres especiais\n- **Impacto**: Texto \"id√™ntico\" vira texto \"diferente\" ap√≥s normaliza√ß√£o\n- **Solu√ß√£o**: Revisar fun√ß√£o `normalize_text()` para consist√™ncia\n\n### üîç Como Interpretar os Resultados dos Testes\n\nExecute as c√©lulas de teste acima e compare:\n\n| Score M√©dio | Diagn√≥stico | A√ß√£o |\n|------------|-------------|------|\n| **99-100%** | ‚úÖ Sistema perfeito | Nenhuma - aceitar como normal |\n| **95-99%** | ‚úÖ Sistema bom | Considerar aumentar `efSearch` se cr√≠tico |\n| **85-95%** | ‚ö†Ô∏è Problema moderado | Aumentar `search_k` ou implementar reranking |\n| **< 85%** | ‚ùå Problema grave | Revisar normaliza√ß√£o e constru√ß√£o de √≠ndices |\n\n### üí° Recomenda√ß√µes para Produ√ß√£o\n\n1. **Aceitar ~99% como \"100%\"**: Sistemas vetoriais t√™m imprecis√£o inerente\n2. **Usar `confidence` ao inv√©s de score absoluto**: \n   - high (>80%), medium (60-80%), low (<60%)\n3. **Implementar reranking**: Pegar top-50 de busca vetorial, reordenar por match exato de strings\n4. **Combinar vetorial + exato**: Use CEP exato como filtro forte, vetorial para fuzzy matching",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "id": "b42275a8",
   "metadata": {},
   "source": [
    "## 6. Exemplos de Uso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f276b080",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exemplo 1: Busca com todos os campos\n",
    "query = {\n",
    "    \"logradouro\": \"avenida rio branco\",\n",
    "    \"bairro\": \"cidade alta\",\n",
    "    \"cidade\": \"natal\",\n",
    "    \"uf\": \"RN\",\n",
    "    \"cep\": \"59025000\"\n",
    "}\n",
    "\n",
    "result = search_engine.search(query, top_k=3, verbose=True)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1497131",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exemplo 2: Busca sem CEP\n",
    "query = {\n",
    "    \"logradouro\": \"rua das flores\",\n",
    "    \"bairro\": \"centro\",\n",
    "    \"cidade\": \"sao paulo\",\n",
    "    \"uf\": \"SP\"\n",
    "}\n",
    "\n",
    "result = search_engine.search(query, top_k=5)\n",
    "result_dict = json.loads(result)\n",
    "\n",
    "print(f\"\\nüîç Resultados para: {query['logradouro']}, {query['cidade']}/{query['uf']}\\n\")\n",
    "\n",
    "for i, res in enumerate(result_dict['results'], 1):\n",
    "    addr = res['address']\n",
    "    print(f\"{i}. Score: {res['score']:.2%} - {res['confidence']}\")\n",
    "    print(f\"   {addr['logradouro']}\")\n",
    "    print(f\"   {addr['bairro']} - {addr['cidade']}/{addr['uf']}\")\n",
    "    print(f\"   CEP: {addr['cep']}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a7c8de1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exemplo 3: Busca parcial (apenas logradouro + cidade)\n",
    "query = {\n",
    "    \"logradouro\": \"avenida paulista\",\n",
    "    \"cidade\": \"sao paulo\",\n",
    "    \"uf\": \"SP\"\n",
    "}\n",
    "\n",
    "result = search_engine.search(query, top_k=3)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04c819bd",
   "metadata": {},
   "source": [
    "## 7. Benchmark de Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79e5c578",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testar performance com 50 buscas aleat√≥rias\n",
    "n_searches = 50\n",
    "times = []\n",
    "\n",
    "print(f\"‚è±Ô∏è  Executando {n_searches} buscas...\\n\")\n",
    "\n",
    "for _ in tqdm(range(n_searches), desc=\"Benchmark\"):\n",
    "    sample = df_dne.sample(1).iloc[0]\n",
    "    query = {\n",
    "        'logradouro': sample['logradouro'],\n",
    "        'bairro': sample['bairro'],\n",
    "        'cidade': sample['cidade'],\n",
    "        'uf': sample['uf']\n",
    "    }\n",
    "    \n",
    "    start = time.time()\n",
    "    search_engine.search(query, top_k=5)\n",
    "    times.append(time.time() - start)\n",
    "\n",
    "times_ms = [t * 1000 for t in times]\n",
    "\n",
    "print(f\"\\n{'='*50}\")\n",
    "print(f\"üìä ESTAT√çSTICAS DE PERFORMANCE\")\n",
    "print(f\"{'='*50}\")\n",
    "print(f\"M√©dia:    {np.mean(times_ms):.1f}ms\")\n",
    "print(f\"Mediana:  {np.median(times_ms):.1f}ms\")\n",
    "print(f\"Min:      {np.min(times_ms):.1f}ms\")\n",
    "print(f\"Max:      {np.max(times_ms):.1f}ms\")\n",
    "print(f\"P95:      {np.percentile(times_ms, 95):.1f}ms\")\n",
    "print(f\"P99:      {np.percentile(times_ms, 99):.1f}ms\")\n",
    "print(f\"\\n‚ö° Throughput: ~{1000/np.mean(times_ms):.0f} queries/segundo\")\n",
    "print(f\"{'='*50}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca4b3ad6",
   "metadata": {},
   "source": [
    "## 8. Notas T√©cnicas\n",
    "\n",
    "### Arquitetura\n",
    "- **Modelo**: `paraphrase-multilingual-MiniLM-L12-v2` (384 dims)\n",
    "- **√çndice**: FAISS IndexHNSWFlat (M=32, efSearch=32)\n",
    "- **Filtragem**: search_k aumentado + filtro posterior (search_k √ó 5 quando UF fornecido)\n",
    "- **Similaridade**: Cosseno (vetores L2-normalizados)\n",
    "\n",
    "### Estrat√©gia de Busca\n",
    "1. **Multi-field search**: logradouro, bairro, cidade, CEP\n",
    "2. **Pesos din√¢micos**: ajustados automaticamente\n",
    "3. **Filtragem por UF**: busca 5x mais candidatos, depois filtra\n",
    "4. **Score agregado**: soma ponderada de similaridades\n",
    "\n",
    "### Performance (g4dn.2xlarge)\n",
    "- **Constru√ß√£o inicial**: 5-10min (1.5M registros)\n",
    "- **Carregamento**: ~5s\n",
    "- **Busca p50**: 30-50ms\n",
    "- **Throughput**: ~500-1000 q/s\n",
    "- **Recall HNSW**: ~99.5%\n",
    "\n",
    "### Pr√≥ximos Passos\n",
    "- [ ] Criar API REST com FastAPI\n",
    "- [ ] Implementar cache de resultados\n",
    "- [ ] Adicionar monitoring/logging\n",
    "- [ ] Testar com FAISS GPU para busca (5-10x speedup)\n",
    "- [ ] Deploy em produ√ß√£o com autoscaling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84f24c4b",
   "metadata": {},
   "source": [
    "## 9. Teste Sint√©tico: Por que n√£o 100%?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "189d1ade",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "üß™ TESTE SINT√âTICO: Valida√ß√£o de Score 100%\n",
    "\n",
    "Estrat√©gia:\n",
    "1. Pegar um endere√ßo REAL da base\n",
    "2. Buscar com os dados EXATOS desse endere√ßo\n",
    "3. Verificar se retorna score 100%\n",
    "4. Diagnosticar onde est√° a perda de score\n",
    "\"\"\"\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"üß™ TESTE SINT√âTICO: An√°lise de Score M√°ximo\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# 1. Selecionar um endere√ßo aleat√≥rio da base como \"ground truth\"\n",
    "sample_idx = np.random.choice(len(df_dne))\n",
    "ground_truth = df_dne.iloc[sample_idx]\n",
    "\n",
    "print(f\"\\nüìç Ground Truth (√≠ndice {sample_idx}):\")\n",
    "print(f\"   Logradouro: {ground_truth['logradouro']}\")\n",
    "print(f\"   Bairro: {ground_truth['bairro']}\")\n",
    "print(f\"   Cidade: {ground_truth['cidade']}\")\n",
    "print(f\"   UF: {ground_truth['uf']}\")\n",
    "print(f\"   CEP: {ground_truth['cep']}\")\n",
    "\n",
    "# 2. Criar query EXATA com os dados desse endere√ßo\n",
    "query_exata = {\n",
    "    \"logradouro\": ground_truth['logradouro'],\n",
    "    \"bairro\": ground_truth['bairro'],\n",
    "    \"cidade\": ground_truth['cidade'],\n",
    "    \"uf\": ground_truth['uf'],\n",
    "    \"cep\": ground_truth['cep']\n",
    "}\n",
    "\n",
    "# 3. Buscar\n",
    "print(f\"\\nüîç Buscando com dados EXATOS...\")\n",
    "result_json = search_engine.search(query_exata, top_k=5, search_k=100)\n",
    "result = json.loads(result_json)\n",
    "\n",
    "# 4. Analisar resultado\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"üìä RESULTADO DA BUSCA\")\n",
    "print(f\"{'='*80}\")\n",
    "\n",
    "top_result = result['results'][0]\n",
    "top_score = top_result['score']\n",
    "top_addr = top_result['address']\n",
    "\n",
    "print(f\"\\nüèÜ Top 1 Result:\")\n",
    "print(f\"   Score: {top_score:.4f} ({top_score*100:.2f}%)\")\n",
    "print(f\"   Logradouro: {top_addr['logradouro']}\")\n",
    "print(f\"   Bairro: {top_addr['bairro']}\")\n",
    "print(f\"   Cidade: {top_addr['cidade']}\")\n",
    "print(f\"   UF: {top_addr['uf']}\")\n",
    "print(f\"   CEP: {top_addr['cep']}\")\n",
    "\n",
    "print(f\"\\nüî¨ Field Scores:\")\n",
    "for field, score in top_result['field_scores'].items():\n",
    "    weight = result['weights_used'][field]\n",
    "    contribution = score * weight\n",
    "    print(f\"   {field:12s}: {score:.4f} √ó {weight:.2f} = {contribution:.4f}\")\n",
    "\n",
    "# 5. Verificar se √© o mesmo registro\n",
    "is_exact_match = (\n",
    "    top_addr['logradouro'] == ground_truth['logradouro'] and\n",
    "    top_addr['bairro'] == ground_truth['bairro'] and\n",
    "    top_addr['cidade'] == ground_truth['cidade'] and\n",
    "    top_addr['uf'] == ground_truth['uf'] and\n",
    "    top_addr['cep'] == ground_truth['cep']\n",
    ")\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"‚úÖ DIAGN√ìSTICO\")\n",
    "print(f\"{'='*80}\")\n",
    "\n",
    "if is_exact_match:\n",
    "    print(f\"‚úÖ Retornou o MESMO registro da base\")\n",
    "    \n",
    "    if top_score >= 0.99:\n",
    "        print(f\"‚úÖ Score praticamente perfeito: {top_score:.4f}\")\n",
    "        print(f\"‚úÖ Sistema funcionando corretamente!\")\n",
    "    else:\n",
    "        print(f\"‚ö†Ô∏è  Score abaixo de 100%: {top_score:.4f}\")\n",
    "        print(f\"\\nüîç An√°lise de perda de score:\")\n",
    "        \n",
    "        # Verificar quais campos n√£o deram 1.0\n",
    "        for field, score in top_result['field_scores'].items():\n",
    "            if score < 0.99:\n",
    "                weight = result['weights_used'][field]\n",
    "                loss = (1.0 - score) * weight\n",
    "                print(f\"   ‚ùå {field}: {score:.4f} (perda de {loss:.4f})\")\n",
    "            else:\n",
    "                print(f\"   ‚úÖ {field}: {score:.4f}\")\n",
    "        \n",
    "        print(f\"\\nüí° CAUSA:\")\n",
    "        print(f\"   Embeddings vetoriais t√™m pequena imprecis√£o mesmo para textos id√™nticos\")\n",
    "        print(f\"   Dist√¢ncia L2 entre embedding do mesmo texto != 0.0 (arredondamento)\")\n",
    "else:\n",
    "    print(f\"‚ùå Retornou um registro DIFERENTE!\")\n",
    "    print(f\"\\nüîç Compara√ß√£o:\")\n",
    "    print(f\"\\n   Ground Truth vs Top Result:\")\n",
    "    print(f\"   Logradouro: {ground_truth['logradouro'] == top_addr['logradouro']}\")\n",
    "    print(f\"   Bairro:     {ground_truth['bairro'] == top_addr['bairro']}\")\n",
    "    print(f\"   Cidade:     {ground_truth['cidade'] == top_addr['cidade']}\")\n",
    "    print(f\"   UF:         {ground_truth['uf'] == top_addr['uf']}\")\n",
    "    print(f\"   CEP:        {ground_truth['cep'] == top_addr['cep']}\")\n",
    "    \n",
    "    print(f\"\\nüí° CAUSA:\")\n",
    "    print(f\"   Busca vetorial retornou candidato diferente!\")\n",
    "    print(f\"   Poss√≠vel duplicata na base ou erro no filtro UF\")\n",
    "\n",
    "print(f\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42a52fe7",
   "metadata": {},
   "source": [
    "### 9.1. Teste Detalhado: Busca por Campo Individual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09fe4a8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "üî¨ TESTE CAMPO POR CAMPO: Identificar onde est√° o problema\n",
    "\n",
    "Para cada campo, vamos:\n",
    "1. Buscar usando apenas aquele campo\n",
    "2. Verificar se o ground truth aparece nos resultados\n",
    "3. Ver qual posi√ß√£o (rank) ele aparece\n",
    "4. Verificar o score de similaridade\n",
    "\"\"\"\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"üî¨ AN√ÅLISE CAMPO POR CAMPO\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Usar o mesmo ground truth do teste anterior\n",
    "print(f\"\\nüìç Ground Truth (√≠ndice {sample_idx}):\")\n",
    "print(f\"   {ground_truth['logradouro']}, {ground_truth['bairro']}\")\n",
    "print(f\"   {ground_truth['cidade']}/{ground_truth['uf']}, CEP: {ground_truth['cep']}\")\n",
    "\n",
    "# Testar cada campo individualmente\n",
    "for field in ['logradouro', 'bairro', 'cidade']:\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"üìå Campo: {field.upper()}\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    query_text = ground_truth[field]\n",
    "    print(f\"   Query: '{query_text}'\")\n",
    "    \n",
    "    # Gerar embedding\n",
    "    query_emb = search_engine.embedding_service.embed_text(query_text)\n",
    "    \n",
    "    # Buscar (com filtro UF)\n",
    "    similarities, indices = search_engine._calculate_field_similarity(\n",
    "        field, query_emb, ground_truth['uf'], top_k=500\n",
    "    )\n",
    "    \n",
    "    # Filtrar por UF\n",
    "    valid_results = []\n",
    "    for idx, sim in zip(indices, similarities):\n",
    "        if idx == -1:\n",
    "            continue\n",
    "        if df_dne.iloc[idx]['uf'] == ground_truth['uf']:\n",
    "            valid_results.append((idx, sim))\n",
    "    \n",
    "    print(f\"\\n   Total de resultados no {ground_truth['uf']}: {len(valid_results)}\")\n",
    "    \n",
    "    # Verificar se o ground truth est√° nos resultados\n",
    "    ground_truth_found = False\n",
    "    ground_truth_rank = -1\n",
    "    ground_truth_sim = 0.0\n",
    "    \n",
    "    for rank, (idx, sim) in enumerate(valid_results[:100], 1):\n",
    "        if idx == sample_idx:\n",
    "            ground_truth_found = True\n",
    "            ground_truth_rank = rank\n",
    "            ground_truth_sim = sim\n",
    "            break\n",
    "    \n",
    "    if ground_truth_found:\n",
    "        print(f\"   ‚úÖ Ground truth ENCONTRADO!\")\n",
    "        print(f\"   üìä Rank: {ground_truth_rank}/100\")\n",
    "        print(f\"   üìä Similaridade: {ground_truth_sim:.6f}\")\n",
    "        \n",
    "        if ground_truth_sim >= 0.9999:\n",
    "            print(f\"   ‚úÖ Similaridade praticamente perfeita!\")\n",
    "        elif ground_truth_sim >= 0.99:\n",
    "            print(f\"   ‚ö†Ô∏è  Similaridade alta mas n√£o perfeita\")\n",
    "        else:\n",
    "            print(f\"   ‚ùå Similaridade abaixo do esperado!\")\n",
    "    else:\n",
    "        print(f\"   ‚ùå Ground truth N√ÉO ENCONTRADO nos top 100!\")\n",
    "        print(f\"   üí° Poss√≠vel causa: texto muito gen√©rico ou duplicatas mais relevantes\")\n",
    "    \n",
    "    # Mostrar top 3 para compara√ß√£o\n",
    "    print(f\"\\n   Top 3 resultados:\")\n",
    "    for rank, (idx, sim) in enumerate(valid_results[:3], 1):\n",
    "        value = df_dne.iloc[idx][field]\n",
    "        is_gt = \" ‚Üê GROUND TRUTH\" if idx == sample_idx else \"\"\n",
    "        print(f\"      {rank}. [idx={idx}] sim={sim:.6f} | '{value[:50]}'{is_gt}\")\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"üí° CONCLUS√ÉO\")\n",
    "print(f\"{'='*80}\")\n",
    "print(f\"Se o ground truth aparece em rank 1 com similaridade ~1.0 em todos os campos,\")\n",
    "print(f\"mas o score final √© <100%, o problema √© que os campos est√£o retornando\")\n",
    "print(f\"√çNDICES DIFERENTES (candidatos diferentes por campo).\")\n",
    "print(f\"\\nSe o ground truth n√£o aparece em rank 1, o problema √© na busca vetorial:\")\n",
    "print(f\"- Embeddings de textos id√™nticos t√™m pequena diferen√ßa num√©rica\")\n",
    "print(f\"- HNSW pode n√£o retornar o exato vizinho mais pr√≥ximo (aproxima√ß√£o)\")\n",
    "print(f\"- Textos muito gen√©ricos t√™m muitos candidatos similares\")\n",
    "print(f\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a49305ec",
   "metadata": {},
   "source": [
    "### 9.2. Teste de Interse√ß√£o: Candidatos por Campo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd026597",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "üéØ TESTE DE INTERSE√á√ÉO: O problema dos candidatos diferentes\n",
    "\n",
    "Este teste mostra se os campos est√£o retornando o MESMO candidato\n",
    "ou candidatos DIFERENTES.\n",
    "\"\"\"\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"üéØ AN√ÅLISE DE INTERSE√á√ÉO DE CANDIDATOS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Coletar top 10 candidatos de cada campo\n",
    "field_candidates = {}\n",
    "\n",
    "for field in ['logradouro', 'bairro', 'cidade']:\n",
    "    query_text = ground_truth[field]\n",
    "    query_emb = search_engine.embedding_service.embed_text(query_text)\n",
    "    \n",
    "    # Buscar\n",
    "    similarities, indices = search_engine._calculate_field_similarity(\n",
    "        field, query_emb, ground_truth['uf'], top_k=100\n",
    "    )\n",
    "    \n",
    "    # Filtrar por UF e pegar top 10\n",
    "    valid_indices = []\n",
    "    for idx, sim in zip(indices, similarities):\n",
    "        if idx == -1:\n",
    "            continue\n",
    "        if df_dne.iloc[idx]['uf'] == ground_truth['uf']:\n",
    "            valid_indices.append(idx)\n",
    "            if len(valid_indices) >= 10:\n",
    "                break\n",
    "    \n",
    "    field_candidates[field] = set(valid_indices)\n",
    "\n",
    "# CEP - busca exata\n",
    "cep_candidates = set(df_dne[\n",
    "    (df_dne['cep'] == ground_truth['cep']) & \n",
    "    (df_dne['uf'] == ground_truth['uf'])\n",
    "].index)\n",
    "\n",
    "field_candidates['cep'] = cep_candidates\n",
    "\n",
    "# An√°lise de interse√ß√£o\n",
    "print(f\"\\nüìä Candidatos √∫nicos por campo (Top 10):\")\n",
    "for field, candidates in field_candidates.items():\n",
    "    gt_present = \"‚úÖ\" if sample_idx in candidates else \"‚ùå\"\n",
    "    print(f\"   {field:12s}: {len(candidates):2d} candidatos {gt_present}\")\n",
    "\n",
    "# Interse√ß√£o entre todos os campos\n",
    "all_fields = ['logradouro', 'bairro', 'cidade', 'cep']\n",
    "intersection = field_candidates[all_fields[0]]\n",
    "for field in all_fields[1:]:\n",
    "    intersection = intersection.intersection(field_candidates[field])\n",
    "\n",
    "print(f\"\\nüîó Interse√ß√£o (candidatos em TODOS os campos): {len(intersection)}\")\n",
    "\n",
    "if len(intersection) > 0:\n",
    "    print(f\"\\n‚úÖ √çndices na interse√ß√£o de todos os campos:\")\n",
    "    for idx in sorted(list(intersection))[:5]:\n",
    "        is_gt = \" ‚Üê GROUND TRUTH\" if idx == sample_idx else \"\"\n",
    "        row = df_dne.iloc[idx]\n",
    "        print(f\"   [idx={idx}]{is_gt}\")\n",
    "        print(f\"      {row['logradouro'][:50]}\")\n",
    "        print(f\"      {row['bairro'][:30]} - {row['cidade']}/{row['uf']}\")\n",
    "        print(f\"      CEP: {row['cep']}\\n\")\n",
    "    \n",
    "    if sample_idx in intersection:\n",
    "        print(f\"   ‚úÖ Ground truth EST√Å na interse√ß√£o!\")\n",
    "        print(f\"   üí° Score deve ser pr√≥ximo de 100%\")\n",
    "    else:\n",
    "        print(f\"   ‚ö†Ô∏è  Ground truth N√ÉO est√° na interse√ß√£o\")\n",
    "        print(f\"   üí° Candidatos perfeitos existem mas n√£o √© o ground truth\")\n",
    "else:\n",
    "    print(f\"\\n‚ùå PROBLEMA: Nenhum candidato aparece em todos os campos!\")\n",
    "    \n",
    "    # Testar combina√ß√µes parciais\n",
    "    from itertools import combinations\n",
    "    \n",
    "    print(f\"\\n   Analisando interse√ß√µes parciais:\")\n",
    "    for combo in combinations(all_fields, 3):\n",
    "        combo_set = field_candidates[combo[0]]\n",
    "        for field in combo[1:]:\n",
    "            combo_set = combo_set.intersection(field_candidates[field])\n",
    "        \n",
    "        if len(combo_set) > 0:\n",
    "            missing = [f for f in all_fields if f not in combo][0]\n",
    "            gt_in_combo = \"‚úÖ\" if sample_idx in combo_set else \"‚ùå\"\n",
    "            print(f\"   {' + '.join(combo):40s}: {len(combo_set):2d} (falta {missing}) {gt_in_combo}\")\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"üí° DIAGN√ìSTICO FINAL\")\n",
    "print(f\"{'='*80}\")\n",
    "\n",
    "if len(intersection) > 0 and sample_idx in intersection:\n",
    "    print(f\"‚úÖ IDEAL: Ground truth aparece em todos os campos\")\n",
    "    print(f\"   ‚Üí Score deve ser ~100% (pequena perda por imprecis√£o num√©rica)\")\n",
    "elif len(intersection) > 0:\n",
    "    print(f\"‚ö†Ô∏è  PARCIAL: Existem candidatos na interse√ß√£o, mas n√£o o ground truth\")\n",
    "    print(f\"   ‚Üí Sistema encontraria 100% se fosse outro endere√ßo\")\n",
    "    print(f\"   ‚Üí Ground truth tem problema espec√≠fico (duplicata ou texto gen√©rico)\")\n",
    "else:\n",
    "    print(f\"‚ùå PROBLEMA: Cada campo retorna candidatos DIFERENTES\")\n",
    "    print(f\"   ‚Üí Por isso o score final √© soma de partes, n√£o 100%\")\n",
    "    print(f\"   ‚Üí Causa: nomes de ruas/bairros/cidades duplicados em outros registros\")\n",
    "    print(f\"   ‚Üí Solu√ß√£o: aumentar search_k ou usar √≠ndices separados por UF\")\n",
    "\n",
    "print(f\"=\"*80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.12.0)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}