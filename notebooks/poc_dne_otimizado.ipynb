{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0da89045",
   "metadata": {},
   "source": [
    "# POC: Busca Vetorial OTIMIZADA para DNE Real (1.5M+ registros)\n",
    "\n",
    "## üöÄ Otimiza√ß√µes implementadas:\n",
    "\n",
    "### 1. **Modelo de embedding mais leve:**\n",
    "- ‚ùå `neuralmind/bert-base-portuguese-cased` (110M par√¢metros, lento)\n",
    "- ‚úÖ `sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2` (118M par√¢metros, **4x mais r√°pido**)\n",
    "- ‚úÖ Alternativa: `rufimelo/bert-large-portuguese-cased-legal-mlm-sts-v1` (otimizado para portugu√™s)\n",
    "\n",
    "\"### 2. **FAISS IndexHNSWFlat ao inv√©s de IndexFlatL2:**\\n\",\n",
    "    \"- ‚ùå IndexFlatL2: busca exata em TODOS os 1.5M vetores (muito lento)\\n\",\n",
    "    \"- ‚úÖ IndexHNSWFlat: busca em grafo hier√°rquico (HNSW - Hierarchical Navigable Small World)\\n\",\n",
    "    \"- **Speedup: 10-50x mais r√°pido** com precis√£o ~99.5% (melhor que IVF)\\n\",\n",
    "    \"- **Sem treinamento**: apenas adiciona os vetores (mais simples)\\n\",\n",
    "\n",
    "### 3. **Processamento em batches maiores:**\n",
    "- Batch size aumentado de 32 para 128\n",
    "- GPU autom√°tica se dispon√≠vel\n",
    "\n",
    "### 4. **Salvamento de √≠ndices:**\n",
    "- Construir √≠ndices uma vez, reutilizar sempre\n",
    "- Evita reprocessamento dos 1.5M registros\n",
    "\n",
    "    \"## ‚è±Ô∏è Performance esperada:\\n\",\n",
    "    \"- **Constru√ß√£o inicial**: ~15-30min (uma √∫nica vez, sem treinamento)\\n\",\n",
    "    \"- **Busca**: <100ms por query (vs ~5s com Flat)\\n\",\n",
    "    \"- **Precis√£o**: ~99.5% (praticamente id√™ntico ao Flat)\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e23b78c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import re\n",
    "import faiss\n",
    "import pickle\n",
    "import torch\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Optional\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from unidecode import unidecode\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "\n",
    "# Detectar GPU\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"üîß Usando device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35d399b4",
   "metadata": {},
   "source": [
    "## 1. EmbeddingService OTIMIZADO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e2adbee",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmbeddingServiceOptimized:\n",
    "    \"\"\"Servi√ßo otimizado para grandes volumes de dados\"\"\"\n",
    "    \n",
    "    def __init__(self, model_name: str = \"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\"):\n",
    "        \"\"\"\n",
    "        Modelos recomendados por velocidade:\n",
    "        1. paraphrase-multilingual-MiniLM-L12-v2 (mais r√°pido, multilingual)\n",
    "        2. all-MiniLM-L6-v2 (muito r√°pido, ingl√™s mas funciona razo√°vel em PT)\n",
    "        3. rufimelo/bert-large-portuguese-cased-legal-mlm-sts-v1 (portugu√™s, m√©dio)\n",
    "        \"\"\"\n",
    "        print(f\"‚ö° Carregando modelo OTIMIZADO: {model_name}\")\n",
    "        self.model = SentenceTransformer(model_name, device=device)\n",
    "        self.embedding_dim = self.model.get_sentence_embedding_dimension()\n",
    "        print(f\"‚úÖ Dimens√£o: {self.embedding_dim}, Device: {self.model.device}\")\n",
    "    \n",
    "    @staticmethod\n",
    "    def normalize_text(text: str) -> str:\n",
    "        if not text or not isinstance(text, str):\n",
    "            return \"\"\n",
    "        \n",
    "        text = unidecode(text)\n",
    "        text = text.lower()\n",
    "        \n",
    "        replacements = {\n",
    "            r'\\br\\.?\\s': 'rua ',\n",
    "            r'\\bav\\.?\\s': 'avenida ',\n",
    "            r'\\btrav\\.?\\s': 'travessa ',\n",
    "            r'\\balam\\.?\\s': 'alameda ',\n",
    "            r'\\bpca\\.?\\s': 'praca ',\n",
    "            r'\\bjd\\.?\\s': 'jardim ',\n",
    "            r'\\bvl\\.?\\s': 'vila ',\n",
    "        }\n",
    "        \n",
    "        for pattern, replacement in replacements.items():\n",
    "            text = re.sub(pattern, replacement, text)\n",
    "        \n",
    "        text = re.sub(r'[^\\w\\s]', ' ', text)\n",
    "        text = re.sub(r'\\s+', ' ', text).strip()\n",
    "        \n",
    "        return text\n",
    "    \n",
    "    def embed_text(self, text: str) -> np.ndarray:\n",
    "        normalized = self.normalize_text(text)\n",
    "        if not normalized:\n",
    "            return np.zeros(self.embedding_dim, dtype=np.float32)\n",
    "        return self.model.encode(normalized, convert_to_numpy=True, show_progress_bar=False)\n",
    "    \n",
    "    def embed_address_fields(self, address: Dict[str, str]) -> Dict[str, np.ndarray]:\n",
    "        embeddings = {}\n",
    "        for field in ['logradouro', 'bairro', 'cidade']:\n",
    "            text = address.get(field, '')\n",
    "            embeddings[field] = self.embed_text(text)\n",
    "        return embeddings\n",
    "    \n",
    "    def embed_batch(self, texts, batch_size: int = 128) -> np.ndarray:\n",
    "        \"\"\"Batch maior para melhor performance\"\"\"\n",
    "        # Converter Series para lista se necess√°rio\n",
    "        if hasattr(texts, 'tolist'):\n",
    "            texts = texts.tolist()\n",
    "        \n",
    "        normalized_texts = [self.normalize_text(t) for t in texts]\n",
    "        normalized_texts = [t if t else \" \" for t in normalized_texts]\n",
    "        \n",
    "        embeddings = self.model.encode(\n",
    "            normalized_texts, \n",
    "            convert_to_numpy=True, \n",
    "            show_progress_bar=True,\n",
    "            batch_size=batch_size,\n",
    "            device=device\n",
    "        )\n",
    "        return embeddings.astype(np.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5c33999",
   "metadata": {},
   "source": [
    "## 2. IndexBuilder com FAISS IVF (R√ÅPIDO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bdb6d2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class IndexBuilderOptimized:\n",
    "    \"\"\"Construtor de √≠ndices FAISS OTIMIZADO para grandes volumes\"\"\"\n",
    "    \n",
    "    def __init__(self, embedding_service: EmbeddingServiceOptimized):\n",
    "        self.embedding_service = embedding_service\n",
    "        self.indices = {}\n",
    "        self.dataframe = None\n",
    "    \n",
    "    \"    def build_indices(self, df: pd.DataFrame, fields: list = None, use_hnsw: bool = True, M: int = 32) -> dict:\\n\",\n",
    "    \"        \\\"\\\"\\\"\\n\",\n",
    "    \"        Constr√≥i √≠ndices FAISS otimizados\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        Args:\\n\",\n",
    "    \"            use_hnsw: True = IndexHNSWFlat (R√ÅPIDO + PRECISO), False = IndexFlatL2 (lento)\\n\",\n",
    "    \"            M: N√∫mero de conex√µes por n√≥ no grafo (16-64, default=32)\\n\",\n",
    "    \"                - M=16: mais r√°pido, menos preciso (~98%)\\n\",\n",
    "    \"                - M=32: balanceado (~99.5%)\\n\",\n",
    "    \"                - M=64: mais preciso (~99.9%), mais mem√≥ria\\n\",\n",
    "    \"        \\\"\\\"\\\"\\n\",\n",
    "    \"        if fields is None:\\n\",\n",
    "    \"            fields = ['logradouro', 'bairro', 'cidade']\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        self.dataframe = df.copy()\\n\",\n",
    "    \"        n_records = len(df)\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        print(f\\\"üî® Construindo √≠ndices para {n_records:,} endere√ßos\\\")\\n\",\n",
    "    \"        print(f\\\"‚öôÔ∏è  Modo: {'HNSW (R√ÅPIDO + PRECISO)' if use_hnsw else 'Flat (LENTO)'}\\\")\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        for field in fields:\\n\",\n",
    "    \"            print(f\\\"\\\\nüìç Processando campo: {field}\\\")\\n\",\n",
    "    \"            start_time = time.time()\\n\",\n",
    "    \"            \\n\",\n",
    "    \"            texts = df[field].fillna('').astype(str).tolist()\\n\",\n",
    "    \"            embeddings = self.embedding_service.embed_batch(texts, batch_size=128)\\n\",\n",
    "    \"            dimension = embeddings.shape[1]\\n\",\n",
    "    \"            \\n\",\n",
    "    \"            if use_hnsw:\\n\",\n",
    "    \"                # HNSW: grafo hier√°rquico naveg√°vel (sem treinamento!)\\n\",\n",
    "    \"                index = faiss.IndexHNSWFlat(dimension, M)\\n\",\n",
    "    \"                \\n\",\n",
    "    \"                # efSearch: quantos vizinhos considerar na busca\\n\",\n",
    "    \"                # Maior = mais preciso mas mais lento (default=16)\\n\",\n",
    "    \"                index.hnsw.efSearch = 32  # balanceado (16-64)\\n\",\n",
    "    \"                \\n\",\n",
    "    \"                print(f\\\"   üß† Adicionando {n_records:,} vetores ao √≠ndice HNSW...\\\")\\n\",\n",
    "    \"                index.add(embeddings)\\n\",\n",
    "    \"            else:\\n\",\n",
    "    \"                # Flat: busca exata (lento para grandes volumes)\\n\",\n",
    "    \"                index = faiss.IndexFlatL2(dimension)\\n\",\n",
    "    \"                index.add(embeddings)\\n\",\n",
    "    \"            \\n\",\n",
    "    \"            self.indices[field] = index\\n\",\n",
    "    \"            elapsed = time.time() - start_time\\n\",\n",
    "    \"            print(f\\\"   ‚úÖ Conclu√≠do em {elapsed:.1f}s\\\")\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        return self.indices\\n\",\n",
    "    \n",
    "    def save_indices(self, output_dir: str):\n",
    "        \"\"\"Salva √≠ndices para reutiliza√ß√£o\"\"\"\n",
    "        output_path = Path(output_dir)\n",
    "        output_path.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        print(f\"üíæ Salvando √≠ndices em: {output_path}\")\n",
    "        \n",
    "        for field, index in self.indices.items():\n",
    "            index_file = output_path / f\"{field}_index.faiss\"\n",
    "            faiss.write_index(index, str(index_file))\n",
    "            print(f\"   ‚úÖ {field}_index.faiss\")\n",
    "        \n",
    "        df_file = output_path / \"addresses.parquet\"\n",
    "        self.dataframe.to_parquet(df_file, index=False)\n",
    "        print(f\"   ‚úÖ addresses.parquet\")\n",
    "        \n",
    "        metadata = {\n",
    "            'fields': list(self.indices.keys()),\n",
    "            'n_records': len(self.dataframe),\n",
    "            'embedding_dim': self.embedding_service.embedding_dim\n",
    "        }\n",
    "        metadata_file = output_path / \"metadata.pkl\"\n",
    "        with open(metadata_file, 'wb') as f:\n",
    "            pickle.dump(metadata, f)\n",
    "        print(f\"   ‚úÖ metadata.pkl\")\n",
    "        print(f\"\\nüéâ √çndices salvos! Use load_indices() para carregar rapidamente.\")\n",
    "    \n",
    "    def load_indices(self, input_dir: str):\n",
    "        \"\"\"Carrega √≠ndices salvos (MUITO mais r√°pido que reconstruir)\"\"\"\n",
    "        input_path = Path(input_dir)\n",
    "        \n",
    "        print(f\"üìÇ Carregando √≠ndices de: {input_path}\")\n",
    "        start_time = time.time()\n",
    "        \n",
    "        metadata_file = input_path / \"metadata.pkl\"\n",
    "        with open(metadata_file, 'rb') as f:\n",
    "            metadata = pickle.load(f)\n",
    "        \n",
    "        df_file = input_path / \"addresses.parquet\"\n",
    "        self.dataframe = pd.read_parquet(df_file)\n",
    "        \n",
    "        for field in metadata['fields']:\n",
    "            index_file = input_path / f\"{field}_index.faiss\"\n",
    "            self.indices[field] = faiss.read_index(str(index_file))\n",
    "            print(f\"   ‚úÖ {field}\")\n",
    "        \n",
    "        elapsed = time.time() - start_time\n",
    "        print(f\"\\n‚ö° Carregado em {elapsed:.1f}s ({len(self.dataframe):,} registros)\")\n",
    "        \n",
    "        return self.indices, self.dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "813dddca",
   "metadata": {},
   "source": [
    "## 3. SearchEngine (mesma l√≥gica)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd8e307d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SearchEngine:\n",
    "    \"\"\"Motor de busca vetorial com pesos din√¢micos\"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self, \n",
    "        embedding_service: EmbeddingServiceOptimized,\n",
    "        indices: Dict[str, faiss.Index],\n",
    "        dataframe: pd.DataFrame\n",
    "    ):\n",
    "        self.embedding_service = embedding_service\n",
    "        self.indices = indices\n",
    "        self.dataframe = dataframe\n",
    "        \n",
    "        self.base_weights = {\n",
    "            'with_cep': {\n",
    "                'cep': 0.30,\n",
    "                'logradouro': 0.40,\n",
    "                'bairro': 0.20,\n",
    "                'cidade': 0.10\n",
    "            },\n",
    "            'without_cep': {\n",
    "                'logradouro': 0.55,\n",
    "                'bairro': 0.25,\n",
    "                'cidade': 0.20\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        self.use_uf_filter = True\n",
    "        self.confidence_threshold = 0.8\n",
    "    \n",
    "    def _get_dynamic_weights(self, query: Dict[str, str]) -> Dict[str, float]:\n",
    "        has_cep = bool(query.get('cep'))\n",
    "        weights = self.base_weights['with_cep' if has_cep else 'without_cep'].copy()\n",
    "        available_fields = [f for f in ['logradouro', 'bairro', 'cidade'] if query.get(f)]\n",
    "        filtered_weights = {k: v for k, v in weights.items() if k in available_fields or k == 'cep'}\n",
    "        total_weight = sum(filtered_weights.values())\n",
    "        if total_weight > 0:\n",
    "            normalized_weights = {k: v / total_weight for k, v in filtered_weights.items()}\n",
    "        else:\n",
    "            normalized_weights = filtered_weights\n",
    "        return normalized_weights\n",
    "    \n",
    "    def _calculate_field_similarity(\n",
    "        self, \n",
    "        field: str, \n",
    "        query_embedding: np.ndarray, \n",
    "        top_k: int = 100\n",
    "    ) -> tuple:\n",
    "        index = self.indices[field]\n",
    "        query_embedding = query_embedding.reshape(1, -1).astype(np.float32)\n",
    "        distances, indices = index.search(query_embedding, top_k)\n",
    "        similarities = 1.0 / (1.0 + distances[0])\n",
    "        return similarities, indices[0]\n",
    "    \n",
    "    def _calculate_cep_match(self, query_cep: str, db_cep: str) -> float:\n",
    "        if not query_cep or not db_cep:\n",
    "            return 0.0\n",
    "        \n",
    "        query_clean = query_cep.replace('-', '').replace('.', '')\n",
    "        db_clean = db_cep.replace('-', '').replace('.', '')\n",
    "        \n",
    "        if query_clean == db_clean:\n",
    "            return 1.0\n",
    "        \n",
    "        if len(query_clean) >= 5 and len(db_clean) >= 5:\n",
    "            if query_clean[:5] == db_clean[:5]:\n",
    "                return 0.5\n",
    "        \n",
    "        return 0.0\n",
    "    \n",
    "    def search(\n",
    "        self, \n",
    "        query: Dict[str, str], \n",
    "        top_k: int = 5,\n",
    "        search_k: int = 100\n",
    "    ) -> str:\n",
    "        weights = self._get_dynamic_weights(query)\n",
    "        query_embeddings = self.embedding_service.embed_address_fields(query)\n",
    "        \n",
    "        candidate_scores = {}\n",
    "        field_scores_map = {}\n",
    "        \n",
    "        for field in ['logradouro', 'bairro', 'cidade']:\n",
    "            if not query.get(field):\n",
    "                continue\n",
    "            \n",
    "            query_emb = query_embeddings[field]\n",
    "            similarities, indices = self._calculate_field_similarity(field, query_emb, search_k)\n",
    "            weight = weights.get(field, 0.0)\n",
    "            \n",
    "            for idx, sim in zip(indices, similarities):\n",
    "                if self.use_uf_filter and query.get('uf'):\n",
    "                    db_uf = self.dataframe.iloc[idx]['uf']\n",
    "                    if db_uf != query['uf']:\n",
    "                        continue\n",
    "                \n",
    "                if idx not in candidate_scores:\n",
    "                    candidate_scores[idx] = 0.0\n",
    "                    field_scores_map[idx] = {}\n",
    "                \n",
    "                candidate_scores[idx] += weight * sim\n",
    "                field_scores_map[idx][field] = float(sim)\n",
    "        \n",
    "        if query.get('cep'):\n",
    "            cep_weight = weights.get('cep', 0.0)\n",
    "            for idx in candidate_scores.keys():\n",
    "                db_cep = self.dataframe.iloc[idx]['cep']\n",
    "                cep_score = self._calculate_cep_match(query.get('cep'), db_cep)\n",
    "                candidate_scores[idx] += cep_weight * cep_score\n",
    "                field_scores_map[idx]['cep'] = cep_score\n",
    "        \n",
    "        sorted_candidates = sorted(\n",
    "            candidate_scores.items(), \n",
    "            key=lambda x: x[1], \n",
    "            reverse=True\n",
    "        )[:top_k]\n",
    "        \n",
    "        results = []\n",
    "        for idx, score in sorted_candidates:\n",
    "            row = self.dataframe.iloc[idx]\n",
    "            \n",
    "            if score >= self.confidence_threshold:\n",
    "                confidence = \"high\"\n",
    "            elif score >= 0.6:\n",
    "                confidence = \"medium\"\n",
    "            else:\n",
    "                confidence = \"low\"\n",
    "            \n",
    "            result = {\n",
    "                \"address\": {\n",
    "                    \"logradouro\": row['logradouro'],\n",
    "                    \"bairro\": row['bairro'],\n",
    "                    \"cidade\": row['cidade'],\n",
    "                    \"uf\": row['uf'],\n",
    "                    \"cep\": row['cep']\n",
    "                },\n",
    "                \"score\": float(score),\n",
    "                \"confidence\": confidence,\n",
    "                \"field_scores\": field_scores_map.get(idx, {})\n",
    "            }\n",
    "            results.append(result)\n",
    "        \n",
    "        response = {\n",
    "            \"results\": results,\n",
    "            \"query\": query,\n",
    "            \"total_found\": len(results),\n",
    "            \"weights_used\": weights\n",
    "        }\n",
    "        \n",
    "        return json.dumps(response, ensure_ascii=False, indent=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca69678f",
   "metadata": {},
   "source": [
    "## 4. Pipeline: Construir √çndices (EXECUTAR UMA VEZ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0279d4be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carregar DNE\n",
    "dne_path = Path('../data/dne.parquet')\n",
    "print(f\"üìÇ Carregando DNE de: {dne_path}\")\n",
    "df_dne_raw = pd.read_parquet(dne_path)\n",
    "\n",
    "# Mapear colunas\n",
    "column_mapping = {\n",
    "    'logradouro_completo': 'logradouro',\n",
    "    'bairro_completo': 'bairro',\n",
    "    'cidade_completo': 'cidade'\n",
    "}\n",
    "df_dne = df_dne_raw.rename(columns=column_mapping)\n",
    "\n",
    "print(f\"‚úÖ Dataset: {len(df_dne):,} registros\")\n",
    "print(f\"üìä Distribui√ß√£o por UF:\")\n",
    "print(df_dne['uf'].value_counts().head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f90a51ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inicializar servi√ßo de embeddings\n",
    "embedding_service = EmbeddingServiceOptimized(\n",
    "    model_name=\"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07cde05e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construir √≠ndices OTIMIZADOS (demora 15-30min, mas faz UMA VEZ)\n",
    "index_builder = IndexBuilderOptimized(embedding_service)\n",
    "\n",
    "# use_hnsw=True: R√ÅPIDO + PRECISO (99.5% recall, sem treinamento!)\n",
    "# M=32: n√∫mero de conex√µes no grafo (16=r√°pido, 32=balanceado, 64=preciso)\n",
    "indices = index_builder.build_indices(df_dne, use_hnsw=True, M=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "998bcf1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SALVAR √≠ndices para n√£o precisar reconstruir\n",
    "index_builder.save_indices('../data/indices_otimizado')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abbbf244",
   "metadata": {},
   "source": [
    "## 5. Pipeline: Carregar √çndices (R√ÅPIDO - use sempre)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eee3092d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carregar modelo\n",
    "embedding_service = EmbeddingServiceOptimized(\n",
    "    model_name=\"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\"\n",
    ")\n",
    "\n",
    "# Carregar √≠ndices salvos (segundos vs 30min!)\n",
    "index_builder = IndexBuilderOptimized(embedding_service)\n",
    "indices, df_dne = index_builder.load_indices('../data/indices_otimizado')\n",
    "\n",
    "# Inicializar motor de busca\n",
    "search_engine = SearchEngine(embedding_service, indices, df_dne)\n",
    "print(f\"\\nüöÄ Sistema pronto! ({len(df_dne):,} endere√ßos indexados)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4139a590",
   "metadata": {},
   "source": [
    "## 6. Teste de Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2078a98c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exemplo de busca\n",
    "query = {\n",
    "    'logradouro': 'Rua das Flores',\n",
    "    'bairro': 'Centro',\n",
    "    'cidade': 'S√£o Paulo',\n",
    "    'uf': 'SP',\n",
    "    'cep': '01000-000'\n",
    "}\n",
    "\n",
    "print(\"‚è±Ô∏è  Testando performance da busca...\\n\")\n",
    "\n",
    "# Medir tempo\n",
    "start = time.time()\n",
    "result = search_engine.search(query, top_k=5)\n",
    "elapsed = time.time() - start\n",
    "\n",
    "print(f\"‚ö° Busca conclu√≠da em: {elapsed*1000:.1f}ms\")\n",
    "print(f\"\\nResultados:\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a5091db",
   "metadata": {},
   "source": [
    "## 7. Benchmark: M√∫ltiplas buscas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd57e9ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testar 10 buscas consecutivas\n",
    "n_searches = 10\n",
    "times = []\n",
    "\n",
    "print(f\"üî• Executando {n_searches} buscas...\\n\")\n",
    "\n",
    "for i in range(n_searches):\n",
    "    sample = df_dne.sample(1).iloc[0]\n",
    "    query = {\n",
    "        'logradouro': sample['logradouro'],\n",
    "        'cidade': sample['cidade'],\n",
    "        'uf': sample['uf']\n",
    "    }\n",
    "    \n",
    "    start = time.time()\n",
    "    result = search_engine.search(query, top_k=5)\n",
    "    elapsed = time.time() - start\n",
    "    times.append(elapsed)\n",
    "    \n",
    "    print(f\"Busca {i+1}: {elapsed*1000:.1f}ms\")\n",
    "\n",
    "print(f\"\\nüìä Estat√≠sticas:\")\n",
    "print(f\"   M√©dia: {np.mean(times)*1000:.1f}ms\")\n",
    "print(f\"   Mediana: {np.median(times)*1000:.1f}ms\")\n",
    "print(f\"   Min: {np.min(times)*1000:.1f}ms\")\n",
    "print(f\"   Max: {np.max(times)*1000:.1f}ms\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72b2efc6",
   "metadata": {},
   "source": [
    "## üìù Notas de Otimiza√ß√£o\n",
    "\n",
    "### Para ajustar performance vs precis√£o:\n",
    "\n",
    "1. **M** (conex√µes no grafo HNSW):\n",
    "   - M=16: mais r√°pido, ~98% precis√£o\n",
    "   - M=32: balanceado, ~99.5% precis√£o ‚úÖ\n",
    "   - M=64: mais preciso, ~99.9% precis√£o, mais mem√≥ria\n",
    "\n",
    "2. **efSearch** (vizinhos considerados na busca):\n",
    "   - `index.hnsw.efSearch = 16`: muito r√°pido, menos preciso\n",
    "   - `index.hnsw.efSearch = 32`: balanceado ‚úÖ\n",
    "   - `index.hnsw.efSearch = 64`: mais preciso, mais lento\n",
    "\n",
    "3. **Modelo de embedding:**\n",
    "   - `paraphrase-multilingual-MiniLM-L12-v2`: balanceado ‚úÖ\n",
    "   - `all-MiniLM-L6-v2`: MUITO r√°pido\n",
    "   - `neuralmind/bert-base-portuguese-cased`: mais preciso, lento\n",
    "\n",
    "### GPU vs CPU:\n",
    "- GPU: ~10x mais r√°pido na constru√ß√£o\n",
    "- Busca: diferen√ßa menor (FAISS j√° √© otimizado)\n",
    "\n",
    "### Alternativa: FAISS GPU\n",
    "```python\n",
    "import faiss.contrib.torch_utils\n",
    "res = faiss.StandardGpuResources()\n",
    "index_gpu = faiss.index_cpu_to_gpu(res, 0, index)\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
