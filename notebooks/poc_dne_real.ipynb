{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "35cfb2c7",
   "metadata": {},
   "source": [
    "# POC: Busca Vetorial Multi-Campo para DNE Real\n",
    "\n",
    "Sistema de busca inteligente com embeddings separados por campo, scoring dinâmico e filtro por UF.\n",
    "\n",
    "## Arquitetura\n",
    "1. **EmbeddingService**: Normalização + geração de embeddings\n",
    "2. **IndexBuilder**: Construção de índices FAISS por campo\n",
    "3. **SearchEngine**: Busca vetorial com agregação ponderada\n",
    "4. **Dataset Real**: Carrega DNE de `data/dne.parquet`\n",
    "\n",
    "## Features\n",
    "- ✅ Busca multi-campo (logradouro, bairro, cidade)\n",
    "- ✅ Pesos dinâmicos baseados em campos presentes\n",
    "- ✅ Filtro por UF para maior determinismo\n",
    "- ✅ CEP como match exato (não vetorial)\n",
    "- ✅ Normalização de abreviações\n",
    "- ✅ Threshold 0.8 para alta confiança"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e02248e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import re\n",
    "import faiss\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Optional\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from unidecode import unidecode\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56f1e118",
   "metadata": {},
   "source": [
    "## 1. EmbeddingService - Normalização e Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0bbe6a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmbeddingService:\n",
    "    \"\"\"Serviço para normalização e geração de embeddings de endereços\"\"\"\n",
    "    \n",
    "    def __init__(self, model_name: str = \"neuralmind/bert-base-portuguese-cased\"):\n",
    "        \"\"\"Inicializa o serviço de embeddings\"\"\"\n",
    "        print(f\"Carregando modelo: {model_name}\")\n",
    "        self.model = SentenceTransformer(model_name)\n",
    "        self.embedding_dim = self.model.get_sentence_embedding_dimension()\n",
    "    \n",
    "    @staticmethod\n",
    "    def normalize_text(text: str) -> str:\n",
    "        \"\"\"Normaliza texto de endereço brasileiro\"\"\"\n",
    "        if not text or not isinstance(text, str):\n",
    "            return \"\"\n",
    "        \n",
    "        text = unidecode(text)\n",
    "        text = text.lower()\n",
    "        \n",
    "        replacements = {\n",
    "            r'\\br\\.': 'rua',\n",
    "            r'\\bav\\.': 'avenida',\n",
    "            r'\\btrav\\.': 'travessa',\n",
    "            r'\\balam\\.': 'alameda',\n",
    "            r'\\bpca\\.': 'praca',\n",
    "            r'\\bjd\\.': 'jardim',\n",
    "            r'\\bvl\\.': 'vila',\n",
    "            r'\\bcj\\.': 'conjunto',\n",
    "            r'\\bqd\\.': 'quadra',\n",
    "            r'\\blt\\.': 'lote',\n",
    "        }\n",
    "        \n",
    "        for pattern, replacement in replacements.items():\n",
    "            text = re.sub(pattern, replacement, text)\n",
    "        \n",
    "        text = re.sub(r'[^\\w\\s]', ' ', text)\n",
    "        text = re.sub(r'\\s+', ' ', text).strip()\n",
    "        \n",
    "        return text\n",
    "    \n",
    "    def embed_text(self, text: str) -> np.ndarray:\n",
    "        \"\"\"Gera embedding para um texto\"\"\"\n",
    "        normalized = self.normalize_text(text)\n",
    "        if not normalized:\n",
    "            return np.zeros(self.embedding_dim, dtype=np.float32)\n",
    "        return self.model.encode(normalized, convert_to_numpy=True, show_progress_bar=False)\n",
    "    \n",
    "    def embed_address_fields(self, address: Dict[str, str]) -> Dict[str, np.ndarray]:\n",
    "        \"\"\"Gera embeddings para cada campo do endereço\"\"\"\n",
    "        embeddings = {}\n",
    "        for field in ['logradouro', 'bairro', 'cidade']:\n",
    "            text = address.get(field, '')\n",
    "            embeddings[field] = self.embed_text(text)\n",
    "        return embeddings\n",
    "    \n",
    "    def embed_batch(self, texts: list) -> np.ndarray:\n",
    "        \"\"\"Gera embeddings para um lote de textos\"\"\"\n",
    "        normalized_texts = [self.normalize_text(t) for t in texts]\n",
    "        normalized_texts = [t if t else \" \" for t in normalized_texts]\n",
    "        embeddings = self.model.encode(\n",
    "            normalized_texts, \n",
    "            convert_to_numpy=True, \n",
    "            show_progress_bar=True,\n",
    "            batch_size=32\n",
    "        )\n",
    "        return embeddings.astype(np.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc9c648d",
   "metadata": {},
   "source": [
    "## 2. IndexBuilder - Construção de Índices FAISS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15e9e0b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class IndexBuilder:\n",
    "    \"\"\"Construtor de índices FAISS para busca vetorial de endereços\"\"\"\n",
    "    \n",
    "    def __init__(self, embedding_service: EmbeddingService):\n",
    "        self.embedding_service = embedding_service\n",
    "        self.indices = {}\n",
    "        self.dataframe = None\n",
    "    \n",
    "    def build_indices(self, df: pd.DataFrame, fields: list = None) -> dict:\n",
    "        \"\"\"Constrói índices FAISS para cada campo\"\"\"\n",
    "        if fields is None:\n",
    "            fields = ['logradouro', 'bairro', 'cidade']\n",
    "        \n",
    "        self.dataframe = df.copy()\n",
    "        n_records = len(df)\n",
    "        \n",
    "        print(f\"Construindo índices FAISS para {n_records} endereços\")\n",
    "        \n",
    "        for field in fields:\n",
    "            texts = df[field].fillna('').astype(str).tolist()\n",
    "            embeddings = self.embedding_service.embed_batch(texts)\n",
    "            dimension = embeddings.shape[1]\n",
    "            index = faiss.IndexFlatL2(dimension)\n",
    "            index.add(embeddings)\n",
    "            self.indices[field] = index\n",
    "        \n",
    "        return self.indices\n",
    "    \n",
    "    def save_indices(self, output_dir: str):\n",
    "        \"\"\"Salva índices FAISS e dataframe em disco\"\"\"\n",
    "        output_path = Path(output_dir)\n",
    "        output_path.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        for field, index in self.indices.items():\n",
    "            index_file = output_path / f\"{field}_index.faiss\"\n",
    "            faiss.write_index(index, str(index_file))\n",
    "        \n",
    "        df_file = output_path / \"addresses.parquet\"\n",
    "        self.dataframe.to_parquet(df_file, index=False)\n",
    "        \n",
    "        metadata = {\n",
    "            'fields': list(self.indices.keys()),\n",
    "            'n_records': len(self.dataframe),\n",
    "            'embedding_dim': self.embedding_service.embedding_dim\n",
    "        }\n",
    "        metadata_file = output_path / \"metadata.pkl\"\n",
    "        with open(metadata_file, 'wb') as f:\n",
    "            pickle.dump(metadata, f)\n",
    "    \n",
    "    def load_indices(self, input_dir: str):\n",
    "        \"\"\"Carrega índices FAISS e dataframe do disco\"\"\"\n",
    "        input_path = Path(input_dir)\n",
    "        \n",
    "        metadata_file = input_path / \"metadata.pkl\"\n",
    "        with open(metadata_file, 'rb') as f:\n",
    "            metadata = pickle.load(f)\n",
    "        \n",
    "        df_file = input_path / \"addresses.parquet\"\n",
    "        self.dataframe = pd.read_parquet(df_file)\n",
    "        \n",
    "        for field in metadata['fields']:\n",
    "            index_file = input_path / f\"{field}_index.faiss\"\n",
    "            self.indices[field] = faiss.read_index(str(index_file))\n",
    "        \n",
    "        return self.indices, self.dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1c01240",
   "metadata": {},
   "source": [
    "## 3. SearchEngine - Motor de Busca Multi-Campo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81201702",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SearchEngine:\n",
    "    \"\"\"Motor de busca vetorial com pesos dinâmicos por campo\"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self, \n",
    "        embedding_service: EmbeddingService,\n",
    "        indices: Dict[str, faiss.Index],\n",
    "        dataframe: pd.DataFrame\n",
    "    ):\n",
    "        self.embedding_service = embedding_service\n",
    "        self.indices = indices\n",
    "        self.dataframe = dataframe\n",
    "        \n",
    "        self.base_weights = {\n",
    "            'with_cep': {\n",
    "                'cep': 0.30,\n",
    "                'logradouro': 0.40,\n",
    "                'bairro': 0.20,\n",
    "                'cidade': 0.10\n",
    "            },\n",
    "            'without_cep': {\n",
    "                'logradouro': 0.55,\n",
    "                'bairro': 0.25,\n",
    "                'cidade': 0.20\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        self.use_uf_filter = True\n",
    "        \n",
    "        self.confidence_threshold = 0.8\n",
    "    \n",
    "    def _get_dynamic_weights(self, query: Dict[str, str]) -> Dict[str, float]:\n",
    "        \"\"\"Calcula pesos dinâmicos baseado nos campos presentes na query\"\"\"\n",
    "        has_cep = bool(query.get('cep'))\n",
    "        weights = self.base_weights['with_cep' if has_cep else 'without_cep'].copy()\n",
    "        available_fields = [f for f in ['logradouro', 'bairro', 'cidade'] if query.get(f)]\n",
    "        filtered_weights = {k: v for k, v in weights.items() if k in available_fields or k == 'cep'}\n",
    "        total_weight = sum(filtered_weights.values())\n",
    "        if total_weight > 0:\n",
    "            normalized_weights = {k: v / total_weight for k, v in filtered_weights.items()}\n",
    "        else:\n",
    "            normalized_weights = filtered_weights\n",
    "        return normalized_weights\n",
    "    \n",
    "    def _calculate_field_similarity(\n",
    "        self, \n",
    "        field: str, \n",
    "        query_embedding: np.ndarray, \n",
    "        top_k: int = 100\n",
    "    ) -> tuple:\n",
    "        \"\"\"Calcula similaridade para um campo específico\"\"\"\n",
    "        index = self.indices[field]\n",
    "        query_embedding = query_embedding.reshape(1, -1).astype(np.float32)\n",
    "        distances, indices = index.search(query_embedding, top_k)\n",
    "        similarities = 1.0 / (1.0 + distances[0])\n",
    "        return similarities, indices[0]\n",
    "    \n",
    "    def _calculate_cep_match(self, query_cep: str, db_cep: str) -> float:\n",
    "        \"\"\"Calcula match exato ou parcial de CEP\"\"\"\n",
    "        if not query_cep or not db_cep:\n",
    "            return 0.0\n",
    "        \n",
    "        query_clean = query_cep.replace('-', '').replace('.', '')\n",
    "        db_clean = db_cep.replace('-', '').replace('.', '')\n",
    "        \n",
    "        if query_clean == db_clean:\n",
    "            return 1.0\n",
    "        \n",
    "        if len(query_clean) >= 5 and len(db_clean) >= 5:\n",
    "            if query_clean[:5] == db_clean[:5]:\n",
    "                return 0.5\n",
    "        \n",
    "        return 0.0\n",
    "    \n",
    "    def search(\n",
    "        self, \n",
    "        query: Dict[str, str], \n",
    "        top_k: int = 5,\n",
    "        search_k: int = 100\n",
    "    ) -> str:\n",
    "        \"\"\"Realiza busca vetorial com scoring dinâmico\"\"\"\n",
    "        weights = self._get_dynamic_weights(query)\n",
    "        query_embeddings = self.embedding_service.embed_address_fields(query)\n",
    "        \n",
    "        candidate_scores = {}\n",
    "        field_scores_map = {}\n",
    "        \n",
    "        for field in ['logradouro', 'bairro', 'cidade']:\n",
    "            if not query.get(field):\n",
    "                continue\n",
    "            \n",
    "            query_emb = query_embeddings[field]\n",
    "            similarities, indices = self._calculate_field_similarity(field, query_emb, search_k)\n",
    "            weight = weights.get(field, 0.0)\n",
    "            \n",
    "            for idx, sim in zip(indices, similarities):\n",
    "                if self.use_uf_filter and query.get('uf'):\n",
    "                    db_uf = self.dataframe.iloc[idx]['uf']\n",
    "                    if db_uf != query['uf']:\n",
    "                        continue\n",
    "                \n",
    "                if idx not in candidate_scores:\n",
    "                    candidate_scores[idx] = 0.0\n",
    "                    field_scores_map[idx] = {}\n",
    "                \n",
    "                candidate_scores[idx] += weight * sim\n",
    "                field_scores_map[idx][field] = float(sim)\n",
    "        \n",
    "        if query.get('cep'):\n",
    "            cep_weight = weights.get('cep', 0.0)\n",
    "            for idx in candidate_scores.keys():\n",
    "                db_cep = self.dataframe.iloc[idx]['cep']\n",
    "                cep_score = self._calculate_cep_match(query.get('cep'), db_cep)\n",
    "                candidate_scores[idx] += cep_weight * cep_score\n",
    "                field_scores_map[idx]['cep'] = cep_score\n",
    "        \n",
    "        sorted_candidates = sorted(\n",
    "            candidate_scores.items(), \n",
    "            key=lambda x: x[1], \n",
    "            reverse=True\n",
    "        )[:top_k]\n",
    "        \n",
    "        results = []\n",
    "        for idx, score in sorted_candidates:\n",
    "            row = self.dataframe.iloc[idx]\n",
    "            \n",
    "            if score >= self.confidence_threshold:\n",
    "                confidence = \"high\"\n",
    "            elif score >= 0.6:\n",
    "                confidence = \"medium\"\n",
    "            else:\n",
    "                confidence = \"low\"\n",
    "            \n",
    "            result = {\n",
    "                \"address\": {\n",
    "                    \"logradouro\": row['logradouro'],\n",
    "                    \"bairro\": row['bairro'],\n",
    "                    \"cidade\": row['cidade'],\n",
    "                    \"uf\": row['uf'],\n",
    "                    \"cep\": row['cep']\n",
    "                },\n",
    "                \"score\": float(score),\n",
    "                \"confidence\": confidence,\n",
    "                \"field_scores\": field_scores_map.get(idx, {})\n",
    "            }\n",
    "            results.append(result)\n",
    "        \n",
    "        response = {\n",
    "            \"results\": results,\n",
    "            \"query\": query,\n",
    "            \"total_found\": len(results),\n",
    "            \"weights_used\": weights\n",
    "        }\n",
    "        \n",
    "        return json.dumps(response, ensure_ascii=False, indent=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c0863f8",
   "metadata": {},
   "source": [
    "## 4. Carregamento do DNE Real"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aad98ebb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Caminho para o arquivo DNE\n",
    "dne_path = Path('../data/dne.parquet')\n",
    "\n",
    "if not dne_path.exists():\n",
    "    raise FileNotFoundError(\n",
    "        f\"Arquivo DNE não encontrado em: {dne_path}\\n\"\n",
    "        \"Por favor, coloque o arquivo 'dne.parquet' na pasta 'data/'\"\n",
    "    )\n",
    "\n",
    "print(f\"Carregando DNE de: {dne_path}\")\n",
    "df_dne_raw = pd.read_parquet(dne_path)\n",
    "\n",
    "print(f\"\\nDataset carregado: {len(df_dne_raw)} registros\")\n",
    "print(f\"\\nColunas disponíveis: {df_dne_raw.columns.tolist()}\")\n",
    "\n",
    "# Mapear colunas do DNE para formato esperado pelo sistema\n",
    "column_mapping = {\n",
    "    'logradouro_completo': 'logradouro',\n",
    "    'bairro_completo': 'bairro',\n",
    "    'cidade_completo': 'cidade'\n",
    "}\n",
    "\n",
    "# Renomear colunas\n",
    "df_dne = df_dne_raw.rename(columns=column_mapping)\n",
    "\n",
    "print(f\"\\nColunas mapeadas:\")\n",
    "for orig, new in column_mapping.items():\n",
    "    if orig in df_dne_raw.columns:\n",
    "        print(f\"  {orig} → {new}\")\n",
    "\n",
    "# Validar colunas obrigatórias após mapeamento\n",
    "required_cols = ['logradouro', 'bairro', 'cidade', 'uf', 'cep']\n",
    "missing_cols = [col for col in required_cols if col not in df_dne.columns]\n",
    "\n",
    "if missing_cols:\n",
    "    raise ValueError(\n",
    "        f\"Colunas obrigatórias faltando no DNE: {missing_cols}\\n\"\n",
    "        f\"Colunas esperadas no arquivo: ['logradouro_completo', 'bairro_completo', 'cidade_completo', 'uf', 'cep']\"\n",
    "    )\n",
    "\n",
    "print(f\"\\nDistribuição por UF:\")\n",
    "print(df_dne['uf'].value_counts())\n",
    "\n",
    "print(f\"\\nRegistros com bairro vazio: {(df_dne['bairro'].fillna('') == '').sum()}\")\n",
    "print(f\"Registros com logradouro vazio: {(df_dne['logradouro'].fillna('') == '').sum()}\")\n",
    "\n",
    "df_dne.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1747f11",
   "metadata": {},
   "source": [
    "## 5. Pipeline Completo - Execução"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50325498",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_service = EmbeddingService(model_name=\"neuralmind/bert-base-portuguese-cased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baa27a3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "index_builder = IndexBuilder(embedding_service)\n",
    "indices = index_builder.build_indices(df_dne)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5a5ba88",
   "metadata": {},
   "outputs": [],
   "source": [
    "search_engine = SearchEngine(\n",
    "    embedding_service=embedding_service,\n",
    "    indices=indices,\n",
    "    dataframe=df_dne\n",
    ")\n",
    "print(f\"Motor de busca inicializado (threshold: {search_engine.confidence_threshold})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cfbe6fb",
   "metadata": {},
   "source": [
    "## 6. Testes de Busca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81118a03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exemplo de busca - ajuste conforme seus dados reais\n",
    "query_example = {\n",
    "    'logradouro': 'Rua das Flores',\n",
    "    'bairro': 'Centro',\n",
    "    'cidade': 'São Paulo',\n",
    "    'uf': 'SP',\n",
    "    'cep': '01000-000'\n",
    "}\n",
    "\n",
    "print(\"=== Teste de Busca ===\")\n",
    "print(f\"Query: {query_example}\\n\")\n",
    "result = search_engine.search(query_example, top_k=5)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "641bae8f",
   "metadata": {},
   "source": [
    "## 7. Salvar Índices (Opcional)\n",
    "\n",
    "Salva os índices FAISS para reutilização futura sem precisar reconstruir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1b3287f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Descomente para salvar os índices\n",
    "# index_builder.save_indices('../data/indices')\n",
    "# print(\"Índices salvos em: data/indices/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9ca29be",
   "metadata": {},
   "source": [
    "## 8. Carregar Índices Salvos (Opcional)\n",
    "\n",
    "Carrega índices previamente salvos para busca rápida"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93849c21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Descomente para carregar índices salvos\n",
    "# index_builder_loaded = IndexBuilder(embedding_service)\n",
    "# indices_loaded, df_loaded = index_builder_loaded.load_indices('../data/indices')\n",
    "# search_engine_loaded = SearchEngine(embedding_service, indices_loaded, df_loaded)\n",
    "# print(\"Índices carregados com sucesso!\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
